#import "/template/consts.typ"
#import "/template/util.typ"
#import "/template/heading.typ": chapter
#import "/template/components.typ": note
#import "/template/font.typ": sil-pua
#import "/template/lang.typ": french, russian, greek, thai, mandingo, german

#import "/lib/glossary.typ": tr

#chapter[
  // The Unicode Standard
  Unicode æ ‡å‡†
]

// When humans exchange information, we use sentences, words, and - most relevantly for our purposes - letters. But computers don't know anything about letters. Computers only know about numbers, and to be honest, they don't know much about them, either. Because computers are, ultimately, great big collections of electronic switches, they only know about two numbers: zero, if a switch is off, and one, when a switch is on.
äººç±»ä½¿ç”¨è¯­å¥ã€å•è¯å’Œï¼ˆä¸æœ¬ä¹¦æœ€ç›¸å…³çš„ï¼‰å­—æ¯æ¥ä¼ é€’ä¿¡æ¯ã€‚ä½†è®¡ç®—æœºä¸è®¤è¯†å­—æ¯ï¼Œå®ƒåªè®¤è¯†æ•°å­—ã€‚è€Œä¸”è¯´å¥å®è¯ï¼Œå®ƒå¯¹æ•°å­—å…¶å®ä¹Ÿä¸æ˜¯ç‰¹åˆ«æ‡‚ã€‚å› ä¸ºå®é™…ä¸Šè®¡ç®—æœºæ˜¯ç”±å¤§é‡ç”µåŠ¨å¼€å…³å †ç Œè€Œæˆï¼Œè€Œå¼€å…³åªå’Œä¸¤ä¸ªæ•°å­—ç›¸å…³ï¼š0ä»£è¡¨å¼€å…³æ–­å¼€ï¼Œ1ä»£è¡¨å¼€å…³æ¥é€šã€‚

// By lining up a row of switches, we can represent bigger numbers. These days, computers normally line up eight switches in a unit, called a *byte*. With eight switches and two states for each switch, we have $$ 2^8 = 256 $$ possible states in a byte, so we can represent a number between 0 and 255. But still, everything is a number.
é€šè¿‡å°†ä¸€ä¸ªä¸ªå¼€å…³è¿æ¥æˆç”µè·¯ï¼Œå®ƒä»¬èƒ½å¤Ÿè¡¨ç¤ºæ›´å¤§çš„æ•°å­—ã€‚ç›®å‰ï¼Œè®¡ç®—æœºé€šå¸¸ä¼šæŠŠå…«ä¸ªè¿™æ ·çš„å¼€å…³ä½œä¸ºä¸€ä¸ªå•å…ƒï¼Œç§°ä¸ºä¸€ä¸ª*å­—èŠ‚*ã€‚å› ä¸ºè¿™å…«ä¸ªå¼€å…³æ¯ä¸ªéƒ½èƒ½æœ‰ä¸¤ç§çŠ¶æ€ï¼Œä¸€ä¸ªå­—èŠ‚å…±æœ‰ $2^8=256$ ç§å¯èƒ½çš„çŠ¶æ€ï¼Œæ‰€ä»¥å¯ä»¥ç”¨æ¥è¡¨ç¤ºä»0åˆ°255çš„æ•°å­—ã€‚ä½†æ— è®ºå¤šå¤§ï¼Œè®¡ç®—æœºä¸­è¿˜æ˜¯åªæœ‰æ•°å­—ã€‚

// To move from numbers to letters and store text in a computer, we need to agree on a code. We might decide that when we're expecting text, the number 1 means "a", number 2 means "b" and so on. This mapping between numbers and characters is called an *encoding*.
ä¸ºäº†è®©è®¡ç®—æœºèƒ½ç”¨æ•°å­—å‚¨å­˜æ–‡æœ¬ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¢«å…±åŒæ‰¿è®¤çš„ç è¡¨ã€‚æ¯”å¦‚å¯ä»¥è¿™æ ·è§„å®šï¼šæ¯å½“æˆ‘ä»¬éœ€è¦æ–‡æœ¬çš„æ—¶å€™ï¼Œæ•°å­—1å°±ä»£è¡¨aï¼Œæ•°å­—2ä»£è¡¨bï¼Œä»¥æ­¤ç±»æ¨ã€‚è¿™å°±åœ¨æ•°å­—å’Œ#tr[character]ä¹‹é—´å»ºç«‹äº†ä¸€ä¸ªæ˜ å°„ï¼Œæˆ‘ä»¬ç§°è¿™ç§æ˜ å°„ä¸º*#tr[encoding]*ã€‚

// In the earliest days of computers, every manufacturer had their own particular encoding, but they soon realised that data written on one system would not be able to be read on another. There was a need for computer and telecommunications manufacturers to standardize their encodings. One of the earliest and most common encodings was ASCII, the American Standard Code for Information Interchange. First standardized in 1963, this system uses seven of the eight bits (switches) in a byte, giving an available range of $$ 2^7 = 128 $$ characters.
åœ¨è®¡ç®—æœºå‘å±•çš„æ—©æœŸï¼Œæ¯ä¸ªåˆ¶é€ å•†éƒ½æ„é€ äº†è‡ªå·±çš„#tr[encoding]ã€‚ä½†å¾ˆå¿«ä»–ä»¬å°±æ„è¯†åˆ°ï¼Œè¿™æ ·çš„è¯æŸä¸ªç³»ç»Ÿä¸Šçš„ä¿¡æ¯æ— æ³•è¢«å…¶ä»–ç³»ç»Ÿæ­£ç¡®è¯»å–ã€‚è®¡ç®—æœºå’Œé€šä¿¡è¡Œä¸šçš„åˆ¶é€ å•†ä»¬éœ€è¦ä¸€ä¸ªæ ‡å‡†åŒ–çš„#tr[encoding]ã€‚æœ€æ—©çš„å…¬å…±#tr[encoding]æ˜¯åœ¨1963å¹´è¢«æ ‡å‡†åŒ–çš„ASCIIï¼ˆAmerican Standard Code for Information Interchangeï¼Œç¾å›½æ ‡å‡†ä¿¡æ¯äº¤æ¢ç ï¼‰ã€‚å®ƒä½¿ç”¨7ä¸ªæ¯”ç‰¹ï¼ˆä¹Ÿå°±æ˜¯ä¸Šæ–‡è¯´çš„å¼€å…³ï¼‰ï¼Œä¸€å…±å¯ä»¥è¡¨è¾¾ $2^7=128$ ä¸ª#tr[character]ã€‚

// In ASCII, the numbers from 0 to 31 are used to encode "control characters". These do not represent printable information, but give instructions to the devices which use ASCII: start a new line, delete the previous character, start transmission, stop transmission and so on. 32 is a space, and the numbers from 33 to 64 are used to represent a range of non-alphabetic symbols, including the numerals 0 to 9. The numbers from 65 to 127 encode the 26 lower case and 26 upper case letters of the English alphabet, with some more non-alphabetic symbols squeezed in the gap.
åœ¨ ASCII ä¸­ï¼Œä»0åˆ°31çš„æ•°å­—ç”¨æ¥#tr[encoding]â€œæ§åˆ¶å­—ç¬¦â€ã€‚ä»–ä»¬å¹¶ä¸èƒ½æ˜¾ç¤ºå‡ºæ¥ï¼Œè€Œæ˜¯ç”¨äºç»™ä½¿ç”¨ ASCII çš„è®¾å¤‡æä¾›ä¸€äº›æ§åˆ¶æŒ‡ä»¤ã€‚æ¯”å¦‚å¼€å§‹æ–°çš„ä¸€è¡Œã€åˆ é™¤ä¸Šä¸€ä¸ªå­—ç¬¦ã€å¼€å§‹æˆ–åœæ­¢ä¿¡æ¯ä¼ è¾“ç­‰ç­‰ã€‚æ•°å­—32è¡¨ç¤ºç©ºæ ¼ã€‚ä»33åˆ°64æ˜¯ä¸€ç³»åˆ—éå­—æ¯çš„ç¬¦å·ï¼Œå…¶ä¸­åŒ…æ‹¬æ•°å­—0åˆ°9ã€‚ä»65åˆ°127åˆ™#tr[encoding]äº†è‹±æ–‡çš„26ä¸ªå°å†™å­—æ¯å’Œ26ä¸ªå¤§å†™å­—æ¯ï¼Œå¤§å°å†™å­—æ¯ä¹‹é—´å¡«å……äº†ä¸€äº›å…¶ä»–çš„éå­—æ¯ç¬¦å·ã€‚

// But ASCII was, as its name implies, an *American* standard, and for most of the world, 26 lower case and 26 upper case letters is - to use something of a British understatement - not going to be enough. When Europeans needed to exchange data including accented characters, or Russians needed to write files in Cyrillic, or Greeks in Greek, the ASCII standard did not allow them to do so. But on the other hand, ASCII only used seven bits out of the byte, encoding the numbers from 0 to 127. And a byte can store any number from 0 to 255, leaving 127 free code points up for grabs.
ä½†é¡¾åæ€ä¹‰ï¼ŒASCIIç æ˜¯ä¸€ä¸ª*ç¾å›½*æ ‡å‡†ã€‚å¯¹äºä¸–ç•Œä¸Šçš„å…¶ä»–åœ°æ–¹ï¼Œ26ä¸ªå°å†™å’Œå¤§å†™å­—æ¯â€”â€”å³ä½¿ä½¿ç”¨è‹±å›½å¼è½»ææ·¡å†™çš„è¯´æ³•â€”â€”æ˜¯è¿œè¿œä¸å¤Ÿçš„ã€‚å½“æ¬§æ´²äººéœ€è¦åœ¨å­—æ¯ä¸ŠåŠ ä¸ŠéŸ³è°ƒæ—¶ï¼Œå½“ä¿„ç½—æ–¯äººéœ€è¦ç¼–å†™å¸¦æœ‰è¥¿é‡Œå°”å­—æ¯çš„æ–‡ä»¶æ—¶ï¼Œå½“å¸Œè…Šäººæƒ³ç”¨å¸Œè…Šå­—æ¯æ—¶ï¼ŒASCIIç ä¸¥è¯æ‹’ç»äº†ä»–ä»¬ã€‚ä½†å¦ä¸€æ–¹é¢ï¼ŒASCIIç åªä½¿ç”¨äº†ä¸€ä¸ªå­—èŠ‚ä¸­çš„ä¸ƒä¸ªæ¯”ç‰¹ï¼Œä¹Ÿå°±æ˜¯æ•°å­—0åˆ°127ã€‚ä½†ä¸€ä¸ªå­—èŠ‚å¯ä»¥å‚¨å­˜0åˆ°255çš„æ•°å­—ï¼Œäºæ˜¯è¿™å‰©ä¸‹çš„127ä¸ªç©ºä½å¼€å§‹è¢«å„å®¶äº‰æŠ¢ã€‚

// The problem, of course, is that 127 code points was not enough for the West Europeans and the Russians and the Greeks to *all* encode all of the characters that they needed. And so, as in the days of the Judges, all the people did whatever seemed right in their own eyes; every national language group developed their own encoding, jamming whatever characters that they needed into the upper half of the ASCII table. Suddenly, all of the interchange problems that ASCII was meant to solve reappeared. Someone in France might send a message asking for a *tÃªte-Ã -tÃªte*, but his Russian colleague would be left wondering what a *tĞ™te-Ğ®-tĞ™te* might be. But wait! It was worse than that: a Greek PC user might greet someone with a cheery ÎšÎ±Î»Î·Î¼Î­ÏÎ±, but if his friend *happened to be using a Mac*, he would find himselfÂ being wished an Î±ÎºÎ³Î»Î¯ÏÎ± instead.
æ˜¾ç„¶ 127 ä¸ªç©ºä½ä¸è¶³æ”¾ä¸‹è¥¿æ¬§ã€ä¿„ç½—æ–¯ã€å¸Œè…Šç­‰ç­‰å›½å®¶æ‰€éœ€è¦çš„æ‰€æœ‰#tr[character]ã€‚æ¥ä¸‹æ¥ï¼Œå°±åƒå£«å¸ˆæ—¶æœŸé‚£æ ·ï¼Œæ‰€æœ‰äººéƒ½åœ¨åšä»–ä»¬çœ¼ä¸­æ­£ç¡®çš„äº‹ã€‚é€šè¿‡å°†è‡ªå·±éœ€è¦çš„#tr[character]æ”¾åˆ°ASCIIç è¡¨çš„ç©ºä½™é«˜ä½éƒ¨åˆ†ï¼Œæ¯ç§è‡ªç„¶è¯­è¨€çš„ä½¿ç”¨ç¾¤ä½“éƒ½åˆ›é€ äº†ä»–ä»¬è‡ªå·±çš„#tr[encoding]ã€‚çªç„¶ä¹‹é—´ï¼ŒASCIIè¯•å›¾è§£å†³çš„è·¨ç³»ç»Ÿä¿¡æ¯äº¤äº’é—®é¢˜åˆé‡ç°äº†ã€‚å¦‚æœä¸€ä¸ªæ³•å›½äººå°†å†™æœ‰#french[tÃªte-Ã -tÃªte]çš„ä¿¡æ¯å‘ç»™ä»–çš„ä¿„ç½—æ–¯åŒäº‹ï¼Œè¿™ä½åŒäº‹ä¼šçœ‹ç€æ˜¾ç¤ºçš„#russian[tĞ™te-Ğ®-tĞ™te]è€Œæ„Ÿåˆ°è¿·æƒ‘ã€‚ä½†ç­‰ç­‰ï¼Œè¿˜æœ‰å¯èƒ½æ›´ç³Ÿå‘¢ï¼ä¸€ä¸ªå¸Œè…Šæ–‡çš„PCç”¨æˆ·å¯èƒ½ä¼šç”¨#greek[ÎšÎ±Î»Î·Î¼Î­ÏÎ±]æ„‰å¿«åœ°å‘å…¶ä»–äººæ‰“æ‹›å‘¼ï¼Œä½†å¦‚æœä»–çš„æœ‹å‹æ°å¥½ä½¿ç”¨çš„æ˜¯Macçš„è¯ï¼Œä»–æ”¶åˆ°çš„ä¼šæ˜¯#greek[Î±ÎºÎ³Î»Î¯ÏÎ±]ã€‚

// And then the Japanese showed up.
æ—¥æ–‡ä¹Ÿæ¥æ·»ä¹±äº†ã€‚

// To write Japanese you need 100 syllabic characters and anything between 2,000 and 100,000 Chinese ideographs. Suddenly 127 free code points seems like a drop in the ocean. There are a number of ways that you can solve this problem, and different Japanese computer manufacturers tried all of them. The Shift JIS encoding used two bytes (16 bits, so $$ 2^{16} = 65536 $$ different states) to represent each character; EUC-JP used a variable number of bytes, with the first byte telling you how many bytes in a character; ISO-2022-JP used magic "escape sequences" of characters to jump between ASCII and JIS. Files didn't always tell you what encoding they were using, and so it was a very common experience in Japan to open up a text file and be greeted with a screen full of mis-encoded gibberish. (The Japanese for "mis-encoded gibberish" is *mojibake*.)
ä¸ºäº†ä¹¦å†™æ—¥æ–‡ï¼Œä½ éœ€è¦å¤§çº¦100ä¸ªéŸ³èŠ‚#tr[character]ï¼Œ2000åˆ°100000ä¸ªè¡¨æ„#tr[character]ã€‚çªç„¶ä¸€ä¸‹ï¼Œ127ä¸ªç©ºä½å°±å˜å¾—åƒæ˜¯æ²§æµ·ä¸€ç²Ÿäº†ã€‚æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ—¥æœ¬çš„è®¡ç®—æœºåˆ¶é€ å•†æŠŠå®ƒä»¬å°è¯•äº†ä¸ªéã€‚Shift JIS#tr[encoding]ä½¿ç”¨ä¸¤ä¸ªå­—èŠ‚ï¼ˆ16ä¸ªæ¯”ç‰¹ï¼Œå…±æœ‰ $2^16 = 65536$ ç§çŠ¶æ€ï¼‰æ¥è¡¨ç¤ºæ‰€æœ‰#tr[character]ï¼›EUC-JP #tr[encoding]åˆ™ä½¿ç”¨å¯å˜æ•°é‡çš„å­—èŠ‚ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå­—èŠ‚ç”¨äºæç¤ºè¿™ä¸ª#tr[character]ä¸€å…±ç”¨å‡ ä¸ªå­—èŠ‚è¡¨ç¤ºï¼›ISO-2022-JP#tr[encoding]åˆ™ä½¿ç”¨â€œè½¬ä¹‰åºåˆ—â€æ¥åœ¨ASCIIå’ŒJIS#tr[encoding]ä¹‹é—´æ¥å›è·³è·ƒã€‚ä¸€ä¸ªæ–‡ä»¶ä¸ä¼šå‘Šè¯‰ä½ å®ƒåœ¨ä½¿ç”¨ä»€ä¹ˆ#tr[encoding]ï¼Œæ‰€ä»¥åœ¨æ—¥æœ¬ï¼Œæ‰“å¼€ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ç„¶åè¢«ä¸€å †ä¹±ç å­—ç¬¦å“ä¸€è·³æ˜¯å¾ˆå¸¸è§çš„ã€‚åœ¨æ—¥æœ¬ï¼Œè¿™ç§å› ä¸ºè½¬ç é”™è¯¯äº§ç”Ÿçš„ä¹±ç è¢«å½¢è±¡çš„ç§°ä¸ºæ–‡å­—å¦–æ€ªï¼ˆmojibakeï¼‰ã€‚

// Clearly there was a need for a new encoding; one with a broad enough scope to encode *all* the world's characters, and one which could unify the proliferation of local "standards" into a single, global information processing standard. That encoding is Unicode.
å¾ˆæ˜æ˜¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„#tr[encoding]ã€‚å®ƒéœ€è¦è¶³ä»¥æ”¾ä¸‹ä¸–ç•Œä¸Š*æ‰€æœ‰*#tr[character]ï¼Œä»¥æ­¤æ¥å°†è¿™äº›åœ¨è›®è’æ—¶ä»£è‚†æ„å¢é•¿å‡ºçš„å„ç§å½“åœ°æ ‡å‡†ç»Ÿä¸€æˆä¸€ä¸ªã€‚è¿™ä¸ªå¯ä»¥ç”¨äºå…¨çƒä¿¡æ¯äº¤æ¢çš„#tr[encoding]æ ‡å‡†å°±æ˜¯ Unicodeã€‚

// In 1984, the International Standards Organisation began the task of developing such a global information processing standard. You may sometimes hear Unicode referred to as ISO 10646, which is sort of true. In 1986, developers from Apple and Xerox began discussing a proposed encoding system which they referred to as Unicode. The Unicode working group expanded and developed in parallel to ISO 10646, and in 1991 became formally incorporated as the Unicode Consortium and publishing Unicode 1.0. At this point, ISO essentially gave up trying to do their own thing.
1984å¹´ï¼Œå›½é™…æ ‡å‡†åŒ–ç»„ç»‡å¼€å§‹äº†å»ºç«‹å…¨çƒä¿¡æ¯äº¤æ¢æ ‡å‡†çš„ä»»åŠ¡ã€‚æœ‰æ—¶å€™ä½ ä¼šé‡åˆ°æŠŠUnicodeç§°ä¸ºISO 10646çš„æƒ…å†µï¼Œè¿™åœ¨æŸç§ç¨‹åº¦ä¸Šä¹Ÿæ˜¯å¯¹çš„ã€‚åœ¨1986å¹´ï¼ŒAppleå’Œæ–½ä¹çš„å¼€å‘äººå‘˜å¼€å§‹è®¨è®ºä¸€ä¸ª#tr[encoding]ç³»ç»Ÿçš„è‰æ¡ˆï¼Œä»–ä»¬ç§°å…¶ä¸ºUnicodeã€‚åœ¨ISO 10646çš„æ„å»ºè¿‡ç¨‹ä¸­ï¼ŒUnicodeå·¥ä½œå°ç»„ä¹Ÿåœ¨ä¸æ–­æ‰©å¼ ã€‚åˆ°1991å¹´ï¼Œè¿™ä¸ªå·¥ä½œå°ç»„æ­£å¼æ³¨å†Œä¸ºUnicodeè”ç›Ÿï¼Œå¹¶å‘å¸ƒäº†Unicodeçš„1.0ç‰ˆã€‚åæ¥ISOåŸºæœ¬ä¸Šå°±ä¸å†è¿›è¡Œè‡ªå·±è¿™è¾¹10646æ ‡å‡†çš„ç¼–å†™äº†ã€‚

// This doesn't mean that ISO 10646 is dead. Instead, ISO 10646 is a formal international standard definition of a Universal Coded Character Set, also known as UCS. The UCS is deliberately synchronised with the character-to-codepoint mapping defined in the Unicode Standard, but the work remains formally independent. At the same time, the Unicode Standard defines more than just the character set; it also defines a wide range of algorithms, data processing expectations and other advisory information about dealing with global scripts.
ä½†è¿™å¹¶ä¸æ„å‘³ç€ISO 10646èƒæ­»è…¹ä¸­ã€‚ISO æœ€ç»ˆå°† 10646 å®šä¸ºäº†ä¸€ä¸ªå«åšé€šç”¨#tr[character set]ï¼ˆUniversal Coded Character Setï¼ŒUCSï¼‰çš„æ­£å¼å›½é™…æ ‡å‡†ã€‚UCS æœ‰æ„åœ°å’Œ Unicode æ ‡å‡†ä¸­çš„#tr[character]åˆ°#tr[codepoint]çš„æ˜ å°„åŒæ­¥ï¼Œä½†åœ¨å½¢å¼ä¸Šè¿™ä¸¤ä¸ªå·¥ä½œæ˜¯äº’ç›¸ç‹¬ç«‹çš„ã€‚ä¸è¿‡ Unicode ä¸åªæ˜¯ä¸€ä¸ª#tr[character set]ï¼Œå®ƒä¹Ÿå®šä¹‰äº†ä¸€ç³»åˆ—å¯ä»¥ç”¨äº#tr[global scripts]æ•°æ®çš„æ¨èæµç¨‹ã€å…·ä½“ç®—æ³•å’Œç›¸å…³ç»†èŠ‚ä¿¡æ¯ã€‚

// ## Global Scripts in Unicode
== Unicode ä¸­çš„#tr[global scripts]

// At the time of writing, the Unicode Standard is up to version 9.0, and new scripts and characters are being encoded all the time. The Unicode character set is divided into 17 planes, each covering 65536 code points, for a total of 1,114,112 possible code points. Currently, only 128,327 of those code points have been assigned characters; 137,468 code points (including the whole of the last two planes) are reserved for private use.
åœ¨å†™ä½œæ—¶ï¼ŒUnicode æ ‡å‡†çš„æœ€æ–°ç‰ˆæ˜¯9.0ï¼Œä¸æ–­æœ‰æ–°çš„#tr[scripts]å’Œ#tr[character]è¢«#tr[encoding]è¿›å»ã€‚Unicode#tr[character set]è¢«åˆ†ä¸º17ä¸ªå¹³é¢ï¼Œæ¯ä¸ªå¹³é¢æœ‰65536ä¸ª#tr[codepoint]ï¼Œå…±æœ‰ 1114112 ä¸ªå¯ç”¨#tr[codepoint]ã€‚ç›®å‰ï¼Œåªæœ‰å…¶ä¸­128327ä¸ªè¢«åˆ†é…äº†#tr[character]ï¼Œè¿˜æœ‰ï¼ˆåŒ…æ‹¬æœ€åä¸¤ä¸ªå¹³é¢çš„ï¼‰137468ä¸ª#tr[codepoint]ä¸ºè‡ªå®šä¹‰çš„ç§äººç”¨é€”è€Œä¿ç•™ã€‚

#note[
  // > Private use means that *within an organisation, community or system* you may use these code points to encode any characters you see fit. However, private use characters should not "escape" into the outside world. Some organisations maintain registries of characters they have assigned to private use code points; for example, the SIL linguistic community have encoded 248 characters for their own use. One of these is ï‰¨, LATIN LETTER SMALL CAPITAL L WITH BELT, which they have encoded at position U+F268. But there's nothing to stop another organisation assigning a *different* character to U+F268 within their systems. If allocations start clashing, you lose the whole point of using a common universal character set. So use private use characters... privately.
  ç§äººä½¿ç”¨æ„å‘³ç€ï¼Œåœ¨*ä¸€ä¸ªç»„ç»‡ã€ç¤¾ç¾¤æˆ–ç³»ç»Ÿä¸­*ï¼Œä½ å¯ä»¥æŒ‰ç…§ç¬¦åˆä½ éœ€æ±‚çš„æ–¹å¼éšæ„ä½¿ç”¨è¿™äº›#tr[codepoint]ã€‚ä½†æ˜¯ï¼Œç§ç”¨#tr[character]ä¸èƒ½â€œé€ƒé€¸â€åˆ°å¤–éƒ¨ä¸–ç•Œä¸­ã€‚ä¸€äº›ç»„ç»‡ä¼šç»´æŠ¤ä¸€ä¸ªä»–ä»¬ä½¿ç”¨çš„ç§äºº#tr[codepoint]çš„ç›®å½•ä»¥ä¾›æŸ¥è¯¢ï¼šæ¯”å¦‚SILè¯­è¨€å­¦ç¤¾åŒº#tr[encoding]äº†ä»–ä»¬è‡ªç”¨çš„248ä¸ª#tr[character]ã€‚å…¶ä¸­ä¸€ä¸ªæ˜¯#sil-pua[/*\u{0F268}*/\u{1DF04}]#footnote[è¯‘æ³¨ï¼šå¤§å¤šæ•°SIL PUA#tr[character]å·²ç»è¢«Unicodeæ­£å¼ç¼–å…¥ï¼Œç¼–å…¥åè¿™äº›PUAåŒºçš„#tr[character]å°±ä¼šæ ‡æ³¨ä¸ºå·²å¼ƒç”¨ã€‚ç›®å‰æ”¯æŒSIL PUAåŒºçš„å­—ä½“éƒ½ä½¿ç”¨äº†æŸç§æ ·å¼æ¥æç¤ºU+F268å·²è¢«å¼ƒç”¨ï¼Œä¸ºäº†æ˜¾ç¤ºè¿™ä¸ª#tr[character]çš„å®é™…æ ·å­ï¼Œæ­¤å¤„ä½¿ç”¨çš„å…¶å®æ˜¯å…¶æ­£å¼Unicode#tr[codepoint]U+1DF04ã€‚], LATIN LETTER SMALL CAPITAL L WITH BELTï¼Œä»–ä»¬å°†å…¶#tr[encoding]åœ¨U+F268çš„ä½ç½®ã€‚ä½†è¿™å¹¶ä¸èƒ½é˜»æ­¢å…¶ä»–ç»„ç»‡åœ¨å…¶ç³»ç»Ÿä¸­æŠŠU+F268åˆ†é…ç»™*åˆ«çš„*#tr[character]ã€‚ä¸€æ—¦åˆ†é…å‘ç”Ÿå†²çªï¼Œä½¿ç”¨å…¬å…±çš„é€šç”¨#tr[character set]å°±å¤±å»äº†æ„ä¹‰ã€‚æ‰€ä»¥ç§ç”¨#tr[character]åªèƒ½åœ¨å†…éƒ¨ä½¿ç”¨ã€‚
]

// Most characters live in the first plane, Plane 0, otherwise known as the Basic Multilingual Plane. The BMP is pretty full now - there are only 128 code points left unallocated - but it covers almost all languages in current use. Plane 1 is called the Supplementary Multilingual Plane, and mainly contains historic scripts, symbols and emoji. Lots and lots of emoji. Plane 2 contains extended CJK (Chinese, Japanese and Korean) ideographs with mainly rare and historic characters, while planes 3 through 13 are currently completely unallocated. So Unicode still has a lot of room to grow.
å¤§å¤šæ•°#tr[character]éƒ½åœ¨ç¬¬ä¸€ä¸ªå¹³é¢ä¸Šï¼Œä¹Ÿå°±æ˜¯ç¬¬0å¹³é¢ã€‚å®ƒçš„å¦ä¸€ä¸ªè‘—åçš„åå­—æ˜¯#tr[BMP]ï¼ˆBasic Multilingual Planeï¼ŒBMPï¼‰ã€‚BMPç°åœ¨åŸºæœ¬æ»¡äº†ï¼Œå®ƒåªå‰©ä¸‹æœ€å 128 ä¸ª#tr[codepoint]è¿˜æ²¡è¢«åˆ†é…ã€‚è¿™ä¸€ä¸ªå¹³é¢å·²ç»è¶³ä»¥æ»¡è¶³å½“å‰ç»å¤§æ•°è¯­è¨€çš„éœ€æ±‚ã€‚ç¬¬1å¹³é¢è¢«ç§°ä¸º#tr[SMP]ï¼Œä¸»è¦åŒ…å«å¤ä»£#tr[scripts]ã€ç¬¦å·å’Œemojiã€‚å¾ˆå¤šå¾ˆå¤šçš„emojiã€‚ç¬¬2å¹³é¢åŒ…å«ä¸­æ—¥éŸ©è¡¨æ„æ–‡å­—æ‰©å±•åŒºï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸å¸¸è§çš„æˆ–è€…å¤ä»£#tr[character]ã€‚ç¬¬3åˆ°13å¹³é¢å®Œå…¨æ²¡æœ‰ä½¿ç”¨ï¼ŒUnicodeé‡Œçš„å¯ç”¨ç©ºé—´è¿˜æœ‰å¾ˆå¤šã€‚

// Within each plane, Unicode allocates each writing system a range of codepoints called a *block*. Blocks are not of fixed size, and are not exhaustive - once codepoints are allocated, they can't be moved around, so if new characters from a writing system get added and their block fills up, a separate block somewhere else in the character set will be created. For instance, groups of Latin-like characters have been added on multiple occasions. This means that there are now 17 blocks allocated for different Latin characters; one of them, Latin Extended-B, consists of 208 code points, and contains Latin characters such as Ç¶ (Latin Capital Letter Hwair), letters used in the transcription of Pinyin, and African clicks like U+013C, Çƒ - which may look a lot like an exclamation mark but is actually the ÇƒKung letter Latin Letter Retroflex Click.
åœ¨æ¯ä¸€ä¸ªå¹³é¢å†…ï¼ŒUnicode ä¼šç»™æŸä¸ª#tr[writing system]åˆ†é…ä¸€ç³»åˆ—è¿ç»­çš„#tr[codepoint]ï¼Œè¿™ç§°ä¸º*#tr[block]*ã€‚#tr[block]ä¸æ˜¯å›ºå®šå¤§å°çš„ï¼Œä¹Ÿä¸ä¿è¯å…¨é¢ã€‚ä¹Ÿå°±æ˜¯å®ƒä¸ä¸€å®šå«æœ‰è¿™ä¸ª#tr[writing system]çš„æ‰€æœ‰#tr[character]ï¼Œå› ä¸ºåœ¨#tr[codepoint]åˆ†é…ç»™äº†#tr[character]äº†åï¼Œå®ƒä»¬å°±ä¸èƒ½ç§»åŠ¨äº†ã€‚æ‰€ä»¥å¦‚æœè¿™ä¸ª#tr[block]çš„æ‰€æœ‰ç©ºä½éƒ½ç”¨å®Œäº†ï¼Œè€Œ#tr[writing system]åˆéœ€è¦å¢åŠ åˆ«çš„#tr[character]çš„è¯ï¼Œæˆ‘ä»¬å°±ä¼šä¸ºå®ƒåœ¨#tr[character set]çš„å…¶ä»–ä½ç½®å†å¼€ä¸€ä¸ª#tr[block]ã€‚æ¯”å¦‚ï¼Œæ‹‰ä¸ç³»#tr[character]å°±æ–°å¤šæ¬¡æ–°å¢è¿‡#tr[block]ï¼Œè‡³ä»Šå·²æœ‰17ä¸ªã€‚å…¶ä¸­ä¸€ä¸ª#tr[block]å«åšæ‹‰ä¸æ‰©å±•BåŒºï¼Œå…¶ä¸­åŒ…å«Ç¶ï¼ˆæ‹‰ä¸å¤§å†™å­—æ¯ Hwairï¼‰ï¼Œåœ¨è½¬å†™æ±‰è¯­æ‹¼éŸ³æ—¶ä½¿ç”¨çš„å­—æ¯ï¼ŒU+013C \u{01C3} ä¹‹ç±»è¡¨ç¤ºéæ´²æ­å˜´éŸ³çš„ç¬¦å·ç­‰ã€‚æœ€åè¿™ä¸ªç¬¦å·çœ‹ä¸Šå»åƒä¸€ä¸ªæ„Ÿå¹å·ï¼Œä½†å®ƒå…¶å®æ˜¯ \u{01C3}Kung è¯­è¨€ä¸­çš„ä¸€ä¸ªå‘å·èˆŒæ­å˜´éŸ³çš„å­—æ¯ï¼Œæˆ‘ä»¬åœ¨æ–‡æœ¬è½¬å†™ä¸­ç”¨`Latin Letter Retroflex Click`è¿™ä¸ªç¬¦å·æ¥è¡¨ç¤ºå®ƒã€‚

// > The distinction between Çƒ (Retroflex Click) and ! (exclamation mark) illustrates a fundamental principle of Unicode: encode what you *mean*, not what you *see*. If we were to use the exclamation mark character for both uses just because they were visually identical, we would sow semantic confusion. Keeping the code points separate keeps your data unambiguous.
#note[
  \u{01C3}ï¼ˆå·èˆŒæ­å˜´éŸ³ï¼‰ å’Œ !ï¼ˆæ„Ÿå¹å·ï¼‰ä¹‹é—´çš„åŒºåˆ«æ­ç¤ºäº†Unicodeä¸­çš„ä¸€ä¸ªåŸåˆ™ï¼šæŒ‰*è¯­ä¹‰*è€Œä¸æ˜¯*å¤–å½¢*æ¥å†³å®šæ˜¯å¦#tr[encoding]ã€‚å¦‚æœæˆ‘ä»¬ä»…ä»…å› ä¸ºå®ƒä¿©çœ‹ä¸Šå»ç›¸ä¼¼ï¼Œå°±åœ¨è¿™ä¸¤ä¸ªåœ°æ–¹éƒ½ç”¨æ„Ÿå¹å·ï¼Œè¿™æ ·ä¼šäº§ç”Ÿè®©äººè¿·æƒ‘çš„è¯­ä¹‰ã€‚ä»#tr[codepoint]ä¸Šå°†å®ƒä»¬åˆ†ç¦»å¯ä»¥è®©æ•°æ®ä¸äº§ç”Ÿæ­§ä¹‰ã€‚
]

// Here is the complete list of scripts already encoded in Unicode as of version 9.0: Adlam, Ahom, Anatolian Hieroglyphs, Arabic, Armenian, Avestan, Balinese, Bamum, Bassa Vah, Batak, Bengali, Bhaiksuki, Bopomofo, Brahmi, Braille, Buginese, Buhid, Canadian Aboriginal, Carian, Caucasian Albanian, Chakma, Cham, Cherokee, Common (that is, characters used in multiple scripts), Coptic, Cuneiform, Cypriot, Cyrillic, Deseret, Devanagari, Duployan, Egyptian Hieroglyphs, Elbasan, Ethiopic, Georgian, Glagolitic, Gothic, Grantha, Greek, Gujarati, Gurmukhi, Han (that is, Chinese, Japanese and Korean ideographs), Hangul, Hanunoo, Hatran, Hebrew, Hiragana, Imperial Aramaic, Inscriptional Pahlavi, Inscriptional Parthian, Javanese, Kaithi, Kannada, Katakana, Kayah Li, Kharoshthi, Khmer, Khojki, Khudawadi, Lao, Latin, Lepcha, Limbu, Linear A, Linear B, Lisu, Lycian, Lydian, Mahajani, Malayalam, Mandaic, Manichaean, Marchen, Meetei Mayek, Mende Kikakui, Meroitic Cursive, Meroitic Hieroglyphs, Miao, Modi, Mongolian, Mro, Multani, Myanmar, Nabataean, New Tai Lue, Newa, Nko, Ogham, Ol Chiki, Old Hungarian, Old Italic, Old North Arabian, Old Permic, Old Persian, Old South Arabian, Old Turkic, Oriya, Osage, Osmanya, Pahawh Hmong, Palmyrene, Pau Cin Hau, Phags Pa, Phoenician, Psalter Pahlavi, Rejang, Runic, Samaritan, Saurashtra, Sharada, Shavian, Siddham, SignWriting, Sinhala, Sora Sompeng, Sundanese, Syloti Nagri, Syriac, Tagalog, Tagbanwa, Tai Le, Tai Tham, Tai Viet, Takri, Tamil, Tangut, Telugu, Thaana, Thai, Tibetan, Tifinagh, Tirhuta, Ugaritic, Vai, Warang Citi, Yi.
ä»¥ä¸‹æ˜¯ Unicode 9.0 ä¸­#tr[encoding]äº†çš„æ‰€æœ‰#tr[scripts]ï¼šAdlam, Ahom, Anatolian Hieroglyphs, Arabic, Armenian, Avestan, Balinese, Bamum, Bassa Vah, Batak, Bengali, Bhaiksuki, Bopomofo, Brahmi, Braille, Buginese, Buhid, Canadian Aboriginal, Carian, Caucasian Albanian, Chakma, Cham, Cherokee, Common (that is, characters used in multiple scripts), Coptic, Cuneiform, Cypriot, Cyrillic, Deseret, Devanagari, Duployan, Egyptian Hieroglyphs, Elbasan, Ethiopic, Georgian, Glagolitic, Gothic, Grantha, Greek, Gujarati, Gurmukhi, Han, Hangul, Hanunoo, Hatran, Hebrew, Hiragana, Imperial Aramaic, Inscriptional Pahlavi, Inscriptional Parthian, Javanese, Kaithi, Kannada, Katakana, Kayah Li, Kharoshthi, Khmer, Khojki, Khudawadi, Lao, Latin, Lepcha, Limbu, Linear A, Linear B, Lisu, Lycian, Lydian, Mahajani, Malayalam, Mandaic, Manichaean, Marchen, Meetei Mayek, Mende Kikakui, Meroitic Cursive, Meroitic Hieroglyphs, Miao, Modi, Mongolian, Mro, Multani, Myanmar, Nabataean, New Tai Lue, Newa, Nko, Ogham, Ol Chiki, Old Hungarian, Old Italic, Old North Arabian, Old Permic, Old Persian, Old South Arabian, Old Turkic, Oriya, Osage, Osmanya, Pahawh Hmong, Palmyrene, Pau Cin Hau, Phags Pa, Phoenician, Psalter Pahlavi, Rejang, Runic, Samaritan, Saurashtra, Sharada, Shavian, Siddham, SignWriting, Sinhala, Sora Sompeng, Sundanese, Syloti Nagri, Syriac, Tagalog, Tagbanwa, Tai Le, Tai Tham, Tai Viet, Takri, Tamil, Tangut, Telugu, Thaana, Thai, Tibetan, Tifinagh, Tirhuta, Ugaritic, Vai, Warang Citi, Yiã€‚#footnote[è¯‘æ³¨ï¼šç¿»è¯‘æœ¬æ®µæ„ä¹‰ä¸å¤§ï¼Œå…¶ä¸­çš„ Han å³ä¸ºæ±‰å­—ã€‚å¸Œæœ›äº†è§£å…¶ä»–#tr[scripts]çš„è¯»è€…å¯è‡ªè¡ŒæŸ¥è¯¢ISO 15924ä»£ç åˆ—è¡¨@Unicode.ISO15924ã€‚]

#note[
  // > When you're developing fonts, you will very often need to know how a particular character is encoded and whereabouts it lives in the Unicode standard - that is, its *codepoint*. For example, the Sinhala letter ayanna is at codepoint 3461 (we usually write these in hexadecimal, as 0D85). How did I know that? I could look it up in the [code charts](https://www.unicode.org/charts/), but actually I have a handy application on my computer called [UnicodeChecker](https://earthlingsoft.net/UnicodeChecker/) which not only helps me find characters and their codepoints, but tells me what the Unicode Standard says about those characters, and also what fonts on my system support them. If you're on a Mac, I recommend that; if not, I recommend finding something similar.
  å½“ä½ å¼€å‘å­—ä½“æ—¶ï¼Œä½ ç»å¸¸éœ€è¦çŸ¥é“ä¸€ä¸ªç‰¹å®š#tr[character]æ˜¯å¦‚ä½•#tr[encoding]çš„ï¼Œä¹Ÿå°±æ˜¯å®ƒå¤„äºUnicodeæ ‡å‡†çš„ä»€ä¹ˆä½ç½®ï¼Œè¿™ç§ä½ç½®å°±æ˜¯#tr[codepoint]ã€‚æ¯”å¦‚ï¼Œåƒ§ä¼½ç½—è¯­å­—æ¯ayannaåœ¨#tr[codepoint]3461ï¼ˆæˆ‘ä»¬ä¸€èˆ¬å†™æˆåå…­è¿›åˆ¶ï¼Œ0D85ï¼‰å¤„ã€‚ä»å“ªèƒ½è·å–è¿™ç§ä¿¡æ¯å‘¢ï¼Ÿä½ å¯ä»¥æŸ¥çœ‹Unicode#tr[character]ä»£ç è¡¨@Unicode.UnicodeCharacterï¼Œä½†æˆ‘ä¸€èˆ¬ä¼šåœ¨ç”µè„‘ä¸Šå®‰è£…ä¸€ä¸ªå«åšUnicodeCheckerçš„åº”ç”¨ç¨‹åº@Earthlingsoft.UnicodeChecker.2022ã€‚å®ƒä¸ä»…èƒ½å¾ˆæ–¹ä¾¿åœ°å‘Šè¯‰æˆ‘Unicodeä¸­æ¯ä¸ª#tr[character]çš„å„ç§ä¿¡æ¯ï¼Œè¿˜èƒ½åˆ—å‡ºæˆ‘ç”µè„‘ä¸­æ”¯æŒè¿™ä¸ª#tr[character]çš„æ‰€æœ‰å­—ä½“ã€‚å¦‚æœä½ ä¹Ÿåœ¨ä½¿ç”¨Macç³»ç»Ÿçš„è¯ï¼Œæˆ‘éå¸¸æ¨èå®ƒã€‚å¦‚æœä¸æ˜¯ï¼Œæˆ‘ä¹Ÿæ¨èä½ æ‰¾æ‰¾å¯¹åº”å¹³å°ä¸Šçš„ç±»ä¼¼åº”ç”¨#footnote[è¯‘æ³¨ï¼šUnicodeCheckeråªæ”¯æŒmacOSï¼Œå¦‚æœä½ åœ¨å…¶ä»–ç³»ç»Ÿä¸Šæœ‰æŸ¥è¯¢æ”¯æŒæŸå­—ç¬¦çš„æ‰€æœ‰å­—ä½“çš„éœ€æ±‚ï¼Œè¯‘è€…ä¸ºæ­¤ç¼–å†™è¿‡è·¨å¹³å°çš„å‘½ä»¤è¡Œå·¥å…·fontfor @7sDream.Fontforï¼Œå¯ä¾›ä¸€è¯•ã€‚]ã€‚
]

// What should you do if you are developing resources for a script which is not encoded in Unicode? Well, first you should check whether or not it has already been proposed for inclusion by looking at the [Proposed New Scripts](http://www.unicode.org/pending/pending.html) web site; if not, then you should contact the Unicode mailing list to see if anyone is working on a proposal; then you should contact the [Script Encoding Initiative](http://linguistics.berkeley.edu/sei/), who will help to guide you through the process of preparing a proposal to the Unicode Technical Committee. This is not a quick process; some scripts have been in the "preliminary stage" for the past ten years, while waiting to gather expert opinions on their encoding.
å¦‚æœä½ æƒ³ä¸ºä¸€ä¸ªè¿˜æ²¡#tr[encoding]è¿›Unicodeä¸­çš„#tr[scripts]å¼€å‘å­—ä½“çš„è¯ï¼Œé¦–å…ˆåº”è¯¥æ£€æŸ¥å·²ææ¡ˆ#tr[scripts]#[@Unicode.ProposedNew]é¡µé¢ä¸­æ˜¯å¦å·²ç»æœ‰å…³äºæ­¤#tr[scripts]çš„ææ¡ˆã€‚å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆä½ åº”è¯¥é€šè¿‡Unicodeé‚®ä»¶åˆ—è¡¨è”ç³»ä»–ä»¬ï¼Œçœ‹çœ‹æ˜¯å¦æœ‰äººæ­£åœ¨ç¼–å†™è¿™ä¸ªææ¡ˆã€‚è¦æ˜¯ä¹Ÿæ²¡æœ‰äººæ­£åœ¨ç¼–å†™ææ¡ˆï¼Œä½ å¯ä»¥å°è¯•è”ç³»ä¸€ä¸‹Script Encoding Initiative#[@ScriptEncodingInitiative.ScriptEncoding]ç»„ç»‡ã€‚åœ¨ä»å‰æœŸå‡†å¤‡åˆ°å‘UnicodeæŠ€æœ¯å§”å‘˜ä¼šæå‡ºææ¡ˆçš„æ•´ä¸ªæµç¨‹ä¸­ï¼Œä»–ä»¬éƒ½å¯ä»¥ä¸ºä½ æä¾›å¸®åŠ©ã€‚è¿™ä¸ªæµç¨‹ä¸ä¼šå¾ˆå¿«ï¼Œæœ‰äº›#tr[scripts]åœ¨ç­‰å¾…æ”¶é›†ä¸“å®¶å¯¹å…¶#tr[encoding]æ„è§çš„â€œå‰æœŸé˜¶æ®µâ€åœç•™äº†åå¤šå¹´ã€‚

// ## How data is stored
== æ•°æ®å¦‚ä½•å‚¨å­˜

// When computers store data in eight-bit bytes, representing numbers from 0 to 255, and your character set contains fewer than 255 characters, everything is easy. A character fits within a byte, so each byte in a file represents one character. One number maps to one letter, and you're done.
æˆ‘ä»¬å·²ç»çŸ¥é“ï¼Œè®¡ç®—æœºä½¿ç”¨åŒ…å«8ä¸ªæ¯”ç‰¹çš„å­—èŠ‚æ¥å‚¨å­˜æ•°æ®ï¼Œæ¯ä¸ªå­—èŠ‚å¯ä»¥è¡¨ç¤ºæ•°å­—0åˆ°255ã€‚å¦‚æœæˆ‘ä»¬çš„#tr[character set]ä¸­çš„#tr[character]æ•°é‡å°äº255çš„è¯ï¼Œäº‹æƒ…å°±å˜å¾—å¾ˆç®€å•ï¼šåªéœ€è¦æ¯ä¸ªå­—èŠ‚å‚¨å­˜ç›¸åº”#tr[character]æ‰€å¯¹åº”çš„æ•°å­—å³å¯ã€‚

// But when your character set has a potential 1,112,064 code points, you need a strategy for how you're going to store those code points in bytes of eight bits. This strategy is called a *character encoding*, and the Unicode Standard defines three of them: UTF-8, UTF-16 and UTF-32. (UTF stands for *Unicode Transformation Format*, because you're transforming code points into bytes and vice versa.)
ä½†å½“è¿™ä¸ªå­—ç¬¦é›†æœ‰1112064ä¸ª#tr[codepoint]æ—¶ï¼ŒæŠŠå®ƒä»¬å‚¨å­˜æˆå­—èŠ‚åºåˆ—å°±éœ€è¦ä¸€äº›ç­–ç•¥äº†ã€‚æˆ‘ä»¬å°†å†³å®šå¦‚ä½•æŠŠ#tr[character set]å‚¨å­˜ä¸ºå­—èŠ‚åºåˆ—çš„ç­–ç•¥ç§°ä¸º*#tr[character]#tr[encoding]*ã€‚Unicode æ ‡å‡†ä¸­å®šä¹‰äº†ä¸‰ç§å­—ç¬¦#tr[encoding]ï¼šUTF-8ã€UTF-16å’ŒUTF-32ã€‚è¿™é‡ŒUTFè¡¨ç¤ºUnicodeè½¬æ¢æ ¼å¼ï¼ˆUnicode Transformation Formatï¼‰ï¼Œé¡¾åæ€ä¹‰ï¼Œå®ƒå°±æ˜¯ç”¨äºUnicode#tr[codepoint]å’Œå­—èŠ‚åºåˆ—ä¹‹é—´çš„äº’ç›¸è½¬æ¢çš„ã€‚

#note[
  // > There are a number of other character encodings in use, which are not part of the Standard, such as UTF-7, UTF-EBCDIC and the Chinese Unicode encoding GB18030. If you need them, you'll know about it.
  åœ¨Unicodeæ ‡å‡†ä¹‹å¤–è¿˜æœ‰å¾ˆå¤šå…¶ä»–çš„#tr[character]#tr[encoding]ï¼Œæ¯”å¦‚ UTF-7ã€UTF-EBCDICå’Œä¸­å›½å›½æ ‡ä¸­å®šä¹‰çš„åŸºäºUnicodeçš„#tr[encoding] GB18030ã€‚è¿™äº›#tr[encoding]å¯ä»¥éšç€å®è·µæŒ‰éœ€äº†è§£ã€‚
]

// The names of the character encodings reflect the number of bits used in encoding. It's easiest to start with UTF-32: if you take a group of 32 bits, you have $$ 2^{32} = 4,294,967,296 $$ possible states, which is more than sufficient to represent every character that's ever likely to be in Unicode. Every character is represented as a group of 32 bits, stretched across four 8-bit bytes. To encode a code point in UTF-32, just turn it into binary, pad it out to four bytes, and you're done.
è¿™äº›#tr[character]#tr[encoding]åç§°ä¸­çš„æ•°å­—åæ˜ äº†å®ƒä»¬ä½¿ç”¨å¤šå°‘æ¯”ç‰¹è¿›è¡Œ#tr[encoding]ã€‚æœ€ç®€å•çš„æ˜¯ UTF-32ï¼šå¦‚æœä½ æŒ‰32ä¸ªæ¯”ç‰¹ä¸ºä¸€ç»„ï¼Œåˆ™ä¸€å…±æœ‰ $2^32=42,9496,7294$ ç§ä¸åŒçš„çŠ¶æ€ï¼Œè¿™å®Œå…¨è¶³ä»¥è¡¨ç¤º Unicde ä¸­çš„æ‰€æœ‰#tr[character]äº†ã€‚æŒ‰è¿™ç§æ–¹æ³•ï¼Œæ¯ä¸ª#tr[character]ä½¿ç”¨32ä¸ªæ¯”ç‰¹ï¼Œä¹Ÿå°±æ˜¯4ä¸ªå­—èŠ‚ã€‚å…·ä½“æ¥è¯´ï¼Œè½¬æ¢æ—¶æŠŠæ¯ä¸ªå­—ç¬¦å¯¹åº”çš„#tr[codepoint]æ•°å­—è½¬æ¢æˆäºŒè¿›åˆ¶ï¼Œç„¶åç”¨0å¡«æ»¡ä¸è¶³32æ¯”ç‰¹çš„å‰©ä½™ä½ç½®å³å¯ã€‚

// For example, the character ğŸ… (FATHER CHRISTMAS) lives in Finland, just inside the Arctic circle, and in the Unicode Standard, at codepoint 127877. In binary, this is 11111001110001111, which we can encode in four bytes using UTF-32 as follows:
æ®è¯´åœ£è¯è€äººä½åœ¨èŠ¬å…°çš„ä¸€ä¸ªä¸´è¿‘åŒ—æåœˆçš„åœ°æ–¹ï¼Œä½† #box(image("/fonts/father-christmas.svg", alt: emoji.santa.man, height: 1em), baseline: 0.1em) ä½œä¸ºä¸€ä¸ª#tr[character]ï¼Œå®ƒä½äºUnicodeä¸­çš„127877#tr[codepoint]ä¸Šã€‚æŒ‰ç…§ UTF-32çš„æ–¹æ³•ï¼Œå…ˆæŠŠ127877è½¬æ¢ä¸ºäºŒè¿›åˆ¶`11111001110001111`ï¼Œå†å°†å®ƒ#tr[encoding]æˆå››ä¸ªå­—èŠ‚ï¼Œç»“æœå¦‚ä¸‹ï¼š

#let utf-32-example-table = c => {
  let codepoint = str.to-unicode(c.text)
  let bs = (0, 0, 0, 0)
  while codepoint != 0 {
    bs.insert(4, calc.rem(codepoint, 256))
    codepoint = int(codepoint / 256)
  }
  bs = bs.slice(-4)
  let binary = bs.map(it => if it == 0 {
    ""
  } else {
    str(it, base: 2)
  })
  let padded = binary.map(it => "0" * (8 - it.len()) + it)
  let hex = bs.map(it => str(it, base: 16)).map(it => "0" * (2 - it.len()) + it)
  let decimal = bs.map(str)

  table(
    columns: 5,
    align: (start, end, end, end, end),
    [äºŒè¿›åˆ¶],
    ..binary.map(raw),
    [å‰è¡¥0],
    ..padded.map(raw),
    [åå…­è¿›åˆ¶],
    ..hex.map(raw),
    [åè¿›åˆ¶],
    ..decimal.map(raw),
  )
}

#align(center)[#utf-32-example-table[ğŸ…]]

#note[
  // > Hexadecimal is a number system which is often used in computer work: whereas decimal "rolls over" to the second place after each 9 (8, 9, 10, 11), hexadecimal counts up to fifteen before rolling over (8, 9, A, B, C, D, E, F, 10, 11). This means that two hexadecimal digits can encode numbers from 00 to FF (or 0 to 255 in decimal), which is precisely the same range as one byte.
  åå…­è¿›åˆ¶æ˜¯åœ¨è®¡ç®—æœºè¯­å¢ƒä¸­å¸¸ç”¨çš„ä¸€ç§æ•°å­—è¿›åˆ¶ï¼šæˆ‘ä»¬æ—¥å¸¸ç”¨çš„åè¿›åˆ¶é€¢åè¿›ä¸€ï¼ˆ8ã€9ã€10ã€11ï¼‰ï¼Œåå…­è¿›åˆ¶åŒç†ï¼Œä¼šé€¢åå…­è¿›ä¸€ï¼ˆ8ã€9ã€Aã€Bã€Cã€Dã€Eã€Fã€10ã€11ï¼‰ã€‚ä¸¤ä½çš„åå…­è¿›åˆ¶æ•°å¯ä»¥ä»00æ•°åˆ°FFï¼ˆæŒ‰åè¿›åˆ¶å°±æ˜¯0åˆ°255ï¼‰ï¼Œæ­£å¥½å’Œä¸€ä¸ªå­—èŠ‚çš„æ•°å­—èŒƒå›´ä¸€è‡´ã€‚
]

#note[
  // > There's only one slight complication: whether the bytes should appear in the order `00 01 F3 85` or in reverse order `85 F3 01 00`. By default, UTF-32 stores data "big-end first" (`00 01 F3 85`) but some systems prefer to put the "little-end" first. They let you know that they're doing this by encoding a special character (ZERO WIDTH NO BREAKING SPACE) at the start of the file. How this character is encoded tells you how the rest of the file is laid out: if you see `00 00 FE FF` at the start of the file, we're big-endian, and if the file starts `FF FE 00 00`, we're little-endian. When ZWNBS is used in this way, it's called a BOM (Byte Order Mark) and is not interpreted as the first character of a document.
  è¿˜æœ‰æœ€åä¸€ä¸ªç•¥æ˜¾å¤æ‚çš„æ­¥éª¤ï¼šæˆ‘ä»¬éœ€è¦å†³å®šè¿™å››ä¸ªå­—èŠ‚æ˜¯æŒ‰`00 01 F3 85`è¿˜æ˜¯æŒ‰`85 F3 01 00`çš„é¡ºåºå‡ºç°ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒUTF-32æŒ‰ç…§å¤§ç«¯åºï¼ˆ`00 01 F3 85`ï¼‰å‚¨å­˜æ•°æ®ï¼Œä½†ä¹Ÿæœ‰äº›ç³»ç»Ÿä¼šåå¥½ä½¿ç”¨å°ç«¯åºã€‚ä¸ºäº†åŒºåˆ†è¿™ä¸¤ç§é¡ºåºï¼Œè¿™äº›ç³»ç»Ÿä¼šåœ¨æ–‡ä»¶çš„èµ·å§‹ä½ç½®æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„é›¶å®½ä¸æŠ˜è¡Œç©ºæ ¼ï¼ˆ`ZERO WIDTH NO BREAKING SPACE`ï¼‰#tr[character]ã€‚è¿™ä¸ªå­—ç¬¦å¦‚ä½•è¢«#tr[encoding]å°±è¡¨ç¤ºäº†åç»­æ–‡ä»¶ä½¿ç”¨ä½•ç§é¡ºåºï¼šå¦‚æœä½ åœ¨æ–‡ä»¶å¼€å¤´çœ‹åˆ°`00 00 FE FF`ï¼Œå°±è¡¨ç¤ºåç»­æ–‡ä»¶å†…å®¹ä½¿ç”¨å¤§ç«¯åºï¼›åŒç†ï¼Œ`FF FE 00 00`å°±è¡¨ç¤ºå°ç«¯åºã€‚å½“è¿™ä¸ªç‰¹æ®Šçš„å­—ç¬¦ç”¨äºæç¤ºå‚¨å­˜é¡ºåºæ—¶ï¼Œæˆ‘ä»¬æŠŠå®ƒå«åšå­—èŠ‚é¡ºåºæ ‡è®°ï¼ˆByte Order Markï¼ŒBOMï¼‰ï¼Œæ­¤æ—¶å®ƒå°±ä¸è¢«è§†ä¸ºæ–‡æ¡£å†…å®¹çš„é¦–ä¸ª#tr[character]äº†ã€‚
]

// ### UTF-16
=== UTF-16

// UTF-32 is a very simple and transparent encoding - four bytes is one character, always, and one character is always four bytes - so it's often used as a way of processing Unicode data inside of a program. Data is read in, in whatever character encoding it happens to be, and is silently converted to UTF-32 so that it can be processed efficiently. The program does what it needs to do with it, and then re-encodes it when it's time to write the data out again. Many programming languages already allow you to read and write numbers that are four bytes long, so representing Unicode code points as numbers isn't a problem. (A "wide character", in languages such as C or Python, is a 32-bit wide data type, ideal for processing UTF-32 data.) But UTF-32 is not very efficient. The first byte is always going to be zero, and the top seven bits of the second byte are always going to be zero too. So UTF-32 is not often used as an on-disk storage format: we don't like the idea of spending nearly 50% of our disk space on bytes that are guaranteed to be empty.
UTF-32æ˜¯ä¸€ç§éå¸¸ç®€æ˜çš„#tr[encoding]ï¼Œå®ƒæ°¸è¿œç”¨å››ä¸ªå­—èŠ‚æè¿°ä¸€ä¸ª#tr[character]ã€‚è¿™ä¸€ç‰¹ç‚¹è®©å®ƒç»å¸¸ç”¨äºç¨‹åºçš„å†…éƒ¨æµç¨‹ã€‚ä¸€ä¸ªç¨‹åºçš„è¾“å…¥å¯èƒ½ä½¿ç”¨å„ç§#tr[encoding]ï¼Œä½†åœ¨å¤„ç†å‰å°†å…¶è½¬æ¢ä¸ºUTF-32å¯ä»¥ä½¿åç»­æµç¨‹æ›´åŠ æ–¹ä¾¿é«˜æ•ˆã€‚åœ¨å®Œæˆå¤„ç†åï¼Œå†æ ¹æ®éœ€æ±‚å°†ç»“æœè½¬æ¢æˆéœ€è¦çš„#tr[encoding]è¾“å‡ºã€‚è®¸å¤šç¼–ç¨‹è¯­è¨€éƒ½å…è®¸ç›´æ¥è¯»å†™å†…å­˜ä¸­çš„å››å­—èŠ‚æ•°å­—ï¼Œæ‰€ä»¥ç›´æ¥å°†Unicode#tr[codepoint]ä½œä¸º#tr[encoding]å¹¶æ²¡æœ‰ä»€ä¹ˆé—®é¢˜ã€‚ï¼ˆåœ¨Cå’ŒPythonç­‰è¯­è¨€ä¸­ï¼Œè¿™ç§å¤„ç†Unicodeçš„32æ¯”ç‰¹ç±»å‹ä¹Ÿè¢«ç§°ä½œå®½å­—ç¬¦ã€‚ï¼‰ä½†æ˜¯UTF-32åœ¨ç©ºé—´ä¸Šå¹¶ä¸é«˜æ•ˆï¼Œå› ä¸ºç¬¬ä¸€ä¸ªå­—èŠ‚æ°¸è¿œæ˜¯0ï¼Œè€Œä¸”ç¬¬äºŒä¸ªå­—èŠ‚çš„å‰7ä¸ªæ¯”ç‰¹ä¹Ÿæ°¸è¿œæ˜¯0ã€‚æ‰€ä»¥UTF-32ä¸å¤ªä¼šä½œä¸ºç£ç›˜å­˜å‚¨æ ¼å¼ï¼Œæ¯•ç«Ÿæˆ‘ä»¬ä¸å¸Œæœ›ä¸ºäº†å­˜è¿™äº›0è€Œæµªè´¹è¿‘50%çš„ç©ºé—´ã€‚

// So can we find a compromise where we use fewer bytes but still represent the majority of characters we're likely to use, in a relatively straightforward way? UTF-16 uses a group of 16 bits (2 bytes) instead of 32, on the basis that the first two bytes of UTF-32 are almost always unused. UTF-16 simply drops those upper two bytes, and instead uses two bytes to represent the Unicode characters from 0 to 65535, the characters within the Basic Multilingual Plane. This worked fine for the majority of characters that people were likely to use. (At least, before emoji inflicted themselves upon the world.) If you want to encode the Thai letter *to pa-tak* (à¸) which lives at code point 3599 in the Unicode standard, we write 3599 in binary: `0000111000001111` and get two bytes `0E 0F`, and that's the UTF-16 encoding.
æˆ‘ä»¬èƒ½æ‰¾åˆ°ä¸€ç§ä½¿ç”¨æ›´å°‘çš„å­—èŠ‚ï¼Œä½†ä¾æ—§èƒ½è¡¨è¾¾éœ€è¦ä½¿ç”¨çš„å¤§å¤šæ•°#tr[character]ï¼Œè€Œä¸”è§„åˆ™ç›¸å¯¹ç®€å•çš„#tr[encoding]å—ï¼ŸUTF-16å¯èƒ½æ˜¯ä¸€ä¸ªé€‰æ‹©ã€‚åŸºäºUTF-32çš„å‰ä¸¤ä¸ªå­—èŠ‚åŸºæœ¬å…¨æ˜¯0è¿™ä¸€æƒ…å†µï¼ŒUTF-16é€‰æ‹©åªä½¿ç”¨16ä¸ªæ¯”ç‰¹ï¼ˆ2ä¸ªå­—èŠ‚ï¼‰ä¸ºä¸€ç»„ã€‚UTF-16åŸºæœ¬ä¸Šå°±æ˜¯æŠ›å¼ƒäº†å‰ä¸¤ä¸ªå­—èŠ‚ï¼Œç”¨å‰©ä¸‹çš„ä¸¤ä¸ªæ¥è¡¨ç¤ºUnicodeé‡Œ0åˆ°65535è¿™ä¸€èŒƒå›´â€”â€”ä¹Ÿå°±æ˜¯å‰æ–‡è¯´çš„#tr[BMP]â€”â€”ä¸­çš„#tr[character]ã€‚è¿™å¯¹äºå¤§å¤šæ•°äººæ—¥å¸¸ä½¿ç”¨çš„#tr[character]æ¥è¯´å·²ç»è¶³å¤Ÿäº†ã€‚ï¼ˆè‡³å°‘åœ¨emojiå…¥ä¾µè¿™ä¸ªä¸–ç•Œä¹‹å‰æ˜¯å¤Ÿçš„ã€‚ï¼‰ä½¿ç”¨è¿™ä¸€æ–¹å¼ï¼Œå¦‚æœæˆ‘ä»¬å¸Œæœ›#tr[encoding]æ³°æ–‡å­—æ¯ `to pa-tak`ï¼ˆ#thai[\u{0E0F}]ï¼‰ï¼Œå› ä¸ºå®ƒåœ¨Unicodeæ ‡å‡†ä¸­çš„#tr[codepoint]æ˜¯3599ï¼Œæˆ‘ä»¬åªéœ€è¦å°†3599è½¬æ¢æˆäºŒè¿›åˆ¶å¾—åˆ°`0000111000001111`ï¼Œå†å°†å®ƒå†™æˆ`0E 0F`ä¸¤ä¸ªå­—èŠ‚ï¼Œè¿™å°±æ˜¯UTF-16çš„#tr[encoding]ç»“æœäº†ã€‚

#note[
  // > From now on, we'll represent Unicode codepoints in the standard way: the prefix `U+` to signify a Unicode codepoint, and then the codepoint in hexadecimal. So *to pa-tak* is U+0E0F.
  ä»ç°åœ¨å¼€å§‹ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ä¸€ç§æ ‡å‡†æ ¼å¼æ¥è¡¨ç¤ºUnicode#tr[codepoint]ï¼šå‰ç¼€ `U+` è¡¨ç¤ºè¿™æ˜¯ä¸€ä¸ªUnicode#tr[codepoint]ï¼Œå…¶åå†™ä¸Š#tr[codepoint]æ•°å­—çš„åå…­è¿›åˆ¶å½¢å¼ã€‚æ¯”å¦‚`to pa-tak`å†™ä½œ `U+0E0F`ã€‚
]

// But what if, as in the case of FATHER CHRISTMAS, we want to access code points above 65535? Converting it into binary gives us a number which is three bytes long, and we want to represent all our characters within two bytes.
ä½†å¦‚æœæˆ‘ä»¬éœ€è¦è¡¨ç¤ºç ä½è¶…è¿‡65535çš„#tr[character]ï¼Œæ¯”å¦‚ä¹‹å‰çš„åœ£è¯è€äººemojiï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿå°†å®ƒçš„#tr[codepoint]è½¬æ¢æˆäºŒè¿›åˆ¶ä¹‹åéœ€è¦ä¸‰ä¸ªå­—èŠ‚æ‰èƒ½æ”¾ä¸‹ï¼Œä½†æ˜¯æˆ‘ä»¬åªå¸Œæœ›ä½¿ç”¨ä¸¤ä¸ªå­—èŠ‚ã€‚

// This is where the compromise comes in: to make it easy and efficient to represent characters in the BMP, UTF-16 gives up the ability to easily and efficiently represent characters outside that plane. Instead, it uses a mechanism called *surrogate pairs* to encode a character from the supplementary planes. A surrogate pair is a 2-byte sequence that looks like it ought to be a valid Unicode character, but is actually from a reserved range which represents a move to another plane. So  UTF-16 uses 16 bits for a character inside the BMP, but two 16 bit sequences for those outside; in other words, UTF-16 is *generally* a fixed-width encoding, but in certain circumstances a character can be either two or four bytes.
è¿™å°±æ˜¯æˆ‘ä»¬å¦¥åçš„åœ°æ–¹äº†ã€‚ä¸ºäº†è®©#tr[BMP]ä¸­çš„#tr[character]èƒ½å¤Ÿç®€å•é«˜æ•ˆçš„è¡¨ç¤ºï¼ŒUTF-16ç‰ºç‰²äº†è¡¨ç¤ºæ­¤å¹³é¢ä¹‹å¤–çš„#tr[character]çš„æ•ˆç‡å’Œç®€æ´æ€§ã€‚å®ƒä½¿ç”¨ä¸€ç§å«åš#tr[surrogate pair]çš„æœºåˆ¶æ¥#tr[encoding]å…¶ä»–è¡¥å……å¹³é¢çš„#tr[character]ã€‚#tr[surrogate pair]æ˜¯ä¸€ä¸ªäºŒå­—èŠ‚åºåˆ—ï¼Œå®ƒçœ‹ä¸Šå»åƒæ˜¯æœ‰æ•ˆçš„Unicode#tr[character]ï¼Œä½†å…¶å®å®ƒä½äºä¸€æ®µè¢«é¢„ç•™çš„åŒºé—´ï¼Œç”¨äºè¡¨ç¤ºåœ¨å…¶ä»–å¹³é¢ä¸Šçš„ä½ç½®ç§»åŠ¨ã€‚æ‰€ä»¥UTF-16ä½¿ç”¨16ä¸ªæ¯”ç‰¹è¡¨ç¤ºBMPä¸­çš„#tr[character]ï¼Œä½†å¯¹äºå…¶ä»–å­—ç¬¦åˆ™éœ€è¦ä¸¤ä¸ª16æ¯”ç‰¹æ‰èƒ½è¡¨ç¤ºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒUTF-16*é€šå¸¸æ¥è¯´*å¯ä»¥å½“ä½œæ˜¯å®šå®½#tr[encoding]çš„ã€‚ä½†åœ¨ç‰¹å®šæƒ…å†µä¸‹ï¼Œä¸€ä¸ªå­—ç¬¦å¯èƒ½æ˜¯ç”¨ä¸¤ä¸ªæˆ–å››ä¸ªå­—èŠ‚è¡¨ç¤ºã€‚

// Surrogate pairs work like this:
#tr[surrogate pair]çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

// * First, subtract `0x010000` from the code point you want to encode. Now you have a 20 bit number.
- é¦–å…ˆï¼Œå°†ä½ æƒ³#tr[encoding]çš„#tr[codepoint]å‡å»`0x010000`ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªå¯ä»¥ç”¨20æ¯”ç‰¹è¡¨ç¤ºçš„æ•°å­—ã€‚
// Split the 20 bit number into two 10 bit numbers. Add `0xD800` to the first, to give a number in the range `0xD800..0xDBFF`. Add `0xDC00` to the second, to give a number in the range `0xDC00..0xDFFF`. These are your two 16-bit code blocks.
- å°†è¿™ä¸ª20æ¯”ç‰¹çš„æ•°åˆ†æˆä¸¤ä¸ª10æ¯”ç‰¹çš„æ•°ã€‚#linebreak()ç»™ç¬¬ä¸€ä¸ªæ•°åŠ ä¸Š`0xD800`ï¼Œå¾—åˆ°çš„æ•°åœ¨`0xD800...0xDBFF`èŒƒå›´å†…ã€‚#linebreak()ç»™ç¬¬äºŒä¸ªæ•°åŠ ä¸Š`0xDC00`ï¼Œå¾—åˆ°çš„æ•°åœ¨`0xDC00...0xDFFF`èŒƒå›´å†…ã€‚#linebreak()è¿™å°±æ˜¯æœ€ç»ˆçš„ä¸¤ä¸ª16æ¯”ç‰¹çš„æ•°æ®å—ã€‚

// So for FATHER CHRISTMAS, we start with U+1F385.
è¿˜æ˜¯ä»¥åœ£è¯è€äººä¸ºä¾‹ï¼Œ#tr[codepoint]æ˜¯`U+1F385`ï¼š

/*
* Take away `0x010000` to get `F385`, or `00001111001110000101`.

* Split this into `0000111100 1110000101` or `03C 385`.

* `0xD800` + `0x03C` = `D83C`

* `0xDC00` + `0x385` = `DF85`.
*/
- å‡å»`0x010000`ä¹‹åæˆ‘ä»¬å¾—åˆ°`F385`ï¼Œä¹Ÿå°±æ˜¯`00001111001110000101`ã€‚
- å°†å®ƒåˆ†æˆ`0000111100 1110000101`ï¼Œåå…­è¿›åˆ¶å†™ä½œ`03C 385`ã€‚
- `0xD800` + `0x03C` = `D83C`ã€‚
- `0xDC00` + `0x385` = `DF85`ã€‚

//So FATHER CHRISTMAS in UTF-16 is `D8 3C DF 85`.
æ‰€ä»¥åœ£è¯è€äººçš„UTF-16#tr[encoding]æ˜¯`D8 3C DF 85`ã€‚

#note[
  // > Because most characters in use are in the BMP, and because the surrogate pairs *could* be interpreted as Unicode code points, some software may not bother to interpret surrogate pair processing. The good news is that emoji characters all live in the supplemental plane, which has forced programmers to become more aware of the issue...
  å› ä¸ºå¤§å¤šæ•°#tr[character]éƒ½åœ¨BMPå†…ï¼Œè€Œä¸”#tr[surrogate pair]ä¹Ÿ*å¯ä»¥*è¢«å½“ä½œUnicode#tr[codepoint]æ¥*ç†è§£*ï¼Œæ‰€ä»¥æœ‰äº›è½¯ä»¶ä¼šä¸å¯¹#tr[surrogate pair]è¿›è¡Œé¢å¤–çš„è§£æå¤„ç†ã€‚å¥½æ¶ˆæ¯æ˜¯å› ä¸ºemojiéƒ½ä½äºè¡¥å……å¹³é¢ä¸Šï¼Œè¿™è®©ç¨‹åºå¼€å‘è€…ä¸å¾—ä¸é‡è§†èµ·è¿™ä¸€é—®é¢˜æ¥â€¦â€¦
]

// ### UTF-8
=== UTF-8

// But UTF-16 still uses *two whole bytes* for every ASCII and Western European Latin character, which sadly are the only characters that any programmers actually care about. So of course, that would never do.
ä½†UTF-16ä¾ç„¶ä¼šä½¿ç”¨*æ•´æ•´ä¸¤ä¸ªå­—èŠ‚*æ¥è¡¨ç¤ºæ‰€æœ‰çš„ASCIIå’Œè¥¿æ¬§æ‹‰ä¸#tr[character]ã€‚æ²¡é”™ï¼Œåœ¨è¿™äº›å‡ ä¹æ˜¯ç¨‹åºå¤„ç†ä¸­æœ€å¸¸ç”¨å’Œå…³å¿ƒçš„#tr[character]ä¸Šæµªè´¹ç©ºé—´æ˜¯ç»å¯¹ä¸è¡Œçš„ã€‚

// UTF-8 takes the trade-off introduced by UTF-16 a little further: characters in the ASCII set are represented as single bytes, just as they were originally in ASCII, while code points above 127 are represented using a variable number of bytes: codepoints from `0x80` to `0x7FF` are two bytes, from `0x800` to `0xffff` are three bytes, and higher characters are four bytes.
UTF-8å°†UTF-16åšå‡ºçš„å¦¥åå’Œæƒè¡¡å‘å‰æ›´è¿›äº†ä¸€æ­¥ï¼šASCII#tr[character]åªä½¿ç”¨ä¸€ä¸ªå­—èŠ‚è¡¨ç¤ºï¼Œå°±åƒåŸæœ¬çš„ASCIIç è¡¨ä¸€æ ·ã€‚è¶…è¿‡127çš„#tr[codepoint]ä½¿ç”¨å¯å˜æ•°é‡çš„å­—èŠ‚è¡¨ç¤ºï¼š`0x80`åˆ°`0x7FF`ä½¿ç”¨ä¸¤ä¸ªå­—èŠ‚ï¼Œ`0x800`åˆ°`0xFFFF`ç”¨ä¸‰ä¸ªå­—èŠ‚ï¼Œæ›´å¤§çš„ç”¨å››ä¸ªå­—èŠ‚ã€‚

// The conversion is best done by an existing computer program or library - you shouldn't have to do UTF-8 encoding by hand - but for reference, this is what you do. First, work out how many bytes the encoding is going to need, based on the Unicode code point. Then, convert the code point to binary, split it up and insert it into the pattern below, and pad with leading zeros:
ä½ ä¸éœ€è¦æ‰‹åŠ¨è¿›è¡ŒUTF-8#tr[encoding]ï¼Œè¿™ä¸ªè¿‡ç¨‹æœ€å¥½ä½¿ç”¨ç°æˆçš„ç¨‹åºæˆ–åº“æ¥å¤„ç†ã€‚ä¸è¿‡ä¸ºäº†æä¾›å‚è€ƒï¼Œæˆ‘ä»¬ä¹Ÿç®€å•ä»‹ç»ä¸€ä¸‹ã€‚é¦–å…ˆï¼ŒåŸºäºä½ æƒ³#tr[encoding]çš„#tr[character]çš„#tr[codepoint]ï¼Œæ ¹æ®ä¸Šè¿°çš„èŒƒå›´å†³å®šä¸€å…±éœ€è¦å‡ ä¸ªå­—èŠ‚ã€‚ç„¶åå°†#tr[codepoint]è½¬æ¢ä¸ºäºŒè¿›åˆ¶å¹¶åˆ†ä¸ºå‡ æ®µï¼Œæ’å…¥ä¸‹é¢çš„æ¨¡æ¿ä¸­ï¼ˆè®°å¾—è¡¥0ï¼‰ï¼š

#align(
  center,
  table(
    columns: 5,
    align: (right,) + (center,) * 4,
    [#tr[codepoint]],
    [ç¬¬ä¸€å­—èŠ‚],
    [ç¬¬äºŒå­—èŠ‚],
    [ç¬¬ä¸‰å­—èŠ‚],
    [ç¬¬å››å­—èŠ‚],
    [`0x00-0x7F`],
    [`0xxxxxxx`],
    [],
    [],
    [],
    [`0x80-0x7FF`],
    [`110xxxxx`],
    [`10xxxxxx`],
    [],
    [],
    [`0x800-0xFFFF`],
    [`1110xxxx`],
    [`10xxxxxx`],
    [`10xxxxxx`],
    [],
    [`0x10000-0x10FFFF`],
    [`11110xxx`],
    [`10xxxxxx`],
    [`10xxxxxx`],
    [`10xxxxxx`],
  ),
)

// > Originally UTF-8 allowed sequences up to seven bytes long to encode characters all the way up to `0x7FFFFFFF`, but this was restricted when UTF-8 became an Internet standard to match the range of UTF-16. Once we need to encode more than a million characters in Unicode, UTF-8 will be insufficient. However, we are still some way away from that situation.
#note[
  æœ€åˆçš„UTF-8å…è®¸ç”¨æœ€é•¿7ä¸ªå­—èŠ‚çš„åºåˆ—æ¥#tr[encoding]ç›´åˆ°`0x7FFFFFFF`çš„#tr[character]ï¼Œä½†è¿™ä¸€ç‚¹åœ¨UTF-8æˆä¸ºäº’è”ç½‘æ ‡å‡†æ—¶ï¼Œä¸ºäº†å’ŒUTF-16çš„èŒƒå›´ä¿æŒä¸€è‡´è€Œè¢«é™åˆ¶äº†ã€‚ä¸€æ—¦æˆ‘ä»¬åœ¨Unicodeä¸­å®¹çº³çš„#tr[character]è¶…è¿‡ç™¾ä¸‡ä¸ªä¹‹åï¼ŒUTF-8å°±ä¸èƒ½èƒœä»»äº†ã€‚ä½†è¿˜å¥½ï¼Œè¿™ä¸€æƒ…å†µç¦»æˆ‘ä»¬è¿˜æ¯”è¾ƒè¿œã€‚
]

// FATHER CHRISTMAS is going to take four bytes, because he is above `0x10000`. The binary representation of his codepoint U+1F385 is `11111001110000101`, so, inserting his bits into the pattern from the right, we get:
åœ£è¯è€äºº#tr[character]ç”¨UTF-8#tr[encoding]éœ€è¦å››ä¸ªå­—èŠ‚ï¼Œå› ä¸ºå®ƒæ¯”`0x10000`å¤§ã€‚å…¶#tr[codepoint]`U+1F385`çš„äºŒè¿›åˆ¶è¡¨ç¤ºä¸º`11111001110000101`ã€‚å°†è¿™äº›æ¯”ç‰¹ï¼ˆä»å³å¾€å·¦ï¼‰æ’å…¥ä¸Šè¿°æ¨¡æ¿ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š

#align(
  center,
  table(
    columns: 5,
    align: (right,) + (center,) * 4,
    [`0x10000-0x10FFFF`],
    [`11110xxx`],
    [`10x`*`11111`*],
    [`10`*`001110`*],
    [`10`*`000101`*],
  ),
)

æœ€åï¼Œå°†æœªå¡«å……çš„`x`è¡¥ä¸Š0ï¼Œå¾—åˆ°ï¼š

#align(
  center,
  table(
    columns: 4,
    align: center,
    [`11110`*`000`*],
    [`10`*`011111`*],
    [`10`*`001110`*],
    [`10`*`000101`*],
    [`F0`],
    [`9F`],
    [`8E`],
    [`85`],
  ),
)

// UTF-8 is not a bad trade-off. It's variable width, which means more work to process, but the benefit of that is efficiency - characters don't take up any more bytes than they need to. And the processing work is mitigated by the fact that the initial byte signals how long the byte sequence is. The leading bytes of the sequence also provide an unambiguous synchronisation point for processing software - if you don't recognise where you are inside a byte stream, just back up a maximum of four characters until you see a byte starting `0`, `110`, `1110` or `11110` and go from there.
UTF-8æ‰€åšçš„æƒè¡¡å…¶å®å¹¶ä¸å·®ã€‚å®ƒæ˜¯å˜å®½çš„ï¼Œè¿™ä¹Ÿå°±æ„å‘³ç€å¤„ç†å®ƒæ›´éº»çƒ¦ã€‚ä½†å¥½å¤„æ˜¯å®ƒçš„ç©ºé—´åˆ©ç”¨éå¸¸é«˜æ•ˆï¼š#tr[character]å®é™…ä½¿ç”¨çš„å­—èŠ‚æ•°æ°¸è¿œä¸ä¼šæ¯”å®ƒå®é™…éœ€è¦çš„æ›´å¤šã€‚è®¾è®¡ä¸­çš„ä¸€ä¸ªå°å·§æ€ä¹Ÿå‡è½»äº†å¤„ç†è´Ÿæ‹…ï¼Œé‚£å°±æ˜¯ä»é¦–ä¸ªå­—èŠ‚ä¸­å°±èƒ½çŸ¥é“è¿™ä¸ªå­—èŠ‚åºåˆ—æœ‰å¤šé•¿ã€‚è€Œä¸”é¦–å­—èŠ‚çš„è¿™ç§ç‰¹æ€§ä¹Ÿæä¾›äº†è½¯ä»¶å¤„ç†è¿‡ç¨‹ä¸­æ‰€éœ€çš„æ¸…æ™°çš„åŒæ­¥ä½ç½®ã€‚æ¯”å¦‚å½“ç¨‹åºä¸çŸ¥é“ç›®å‰æ‰€å¤„çš„æ˜¯åºåˆ—ä¸­çš„ç¬¬å‡ ä¸ªå­—èŠ‚æ—¶ï¼Œå®ƒåªéœ€è¦ç®€å•çš„å¾€åçœ‹æœ€å¤šå››ä¸ªå­—èŠ‚ï¼Œåªè¦å‘ç°æŸä¸ªå­—èŠ‚ç”±`0`ã€`110`ã€`1110`æˆ–`11110`å¼€å¤´ï¼Œå°±èƒ½å¤Ÿä»è¿™é‡Œç»§ç»­å¾€ä¸‹å¤„ç†äº†ã€‚

// Because of this, UTF-8 has become the *de facto* encoding standard of the Internet, with around 90% of web pages using it. If you have data that you're going to be shaping with an OpenType shaping engine, it's most likely going to begin in UTF-8 before being transformed to UTF-32 internally.
ç”±äºä¸Šè¿°åŸå› ï¼ŒUTF-8æˆä¸ºäº†äº’è”ç½‘ä¸Šæ–‡æœ¬#tr[encoding]çš„*äº‹å®æ ‡å‡†*ï¼Œå¤§çº¦90%çš„ç½‘é¡µéƒ½ä½¿ç”¨å®ƒã€‚å¦‚æœä½ çš„æ•°æ®éœ€è¦è¢«OpenType#tr[shaping]å¼•æ“å¤„ç†çš„è¯ï¼Œå¾ˆå¯èƒ½å®ƒéœ€è¦æ˜¯UTF-8#tr[encoding]çš„ï¼Œå¼•æ“çš„å†…éƒ¨æµç¨‹ä¼šå°†å®ƒè½¬åŒ–ä¸ºUTF-32å¤„ç†ã€‚

// ## Character properties
== #tr[character]å±æ€§

// The Unicode Standard isn't merely a collection of characters and their code points. The standard also contains the Unicode Character Database, a number of core data files containing the information that computers need in order to correctly process those characters. For example, the main database file, `UnicodeData.txt` contains a `General_Category` property which tells you if a codepoint represents a letter, number, mark, punctuation character and so on.
Unicodeæ ‡å‡†ä¸ä»…åªæ˜¯æ”¶é›†æ‰€æœ‰#tr[character]å¹¶ä¸ºä»–ä»¬åˆ†é…#tr[codepoint]è€Œå·²ï¼Œå®ƒè¿˜å»ºç«‹äº†Unicode#tr[character]æ•°æ®åº“ã€‚è¿™ä¸ªæ•°æ®åº“ä¸­åŒ…å«äº†è®¸å¤šæ ¸å¿ƒæ•°æ®æ–‡ä»¶ï¼Œè®¡ç®—æœºä¾èµ–è¿™äº›æ–‡ä»¶ä¸­çš„ä¿¡æ¯æ¥æ­£ç¡®å¤„ç†#tr[character]ã€‚ä¾‹å¦‚ï¼Œä¸»æ•°æ®æ–‡ä»¶`UnicodeData.txt`ä¸­å®šä¹‰äº†#tr[general category]å±æ€§ï¼Œå®ƒèƒ½å‘Šè¯‰ä½ æŸä¸ª#tr[character]æ˜¯å­—æ¯ã€æ•°å­—ã€æ ‡ç‚¹è¿˜æ˜¯ç¬¦å·ç­‰ç­‰ã€‚

// Let's pick a few characters and see what Unicode says about them. We'll begin with codepoint U+0041, the letter `A`. First, looking in `UnicodeData.txt` we see
æˆ‘ä»¬æŒ‘ä¸€äº›å­—ç¬¦æ¥çœ‹çœ‹Unicodeèƒ½æä¾›å…³äºå®ƒä»¬çš„å“ªäº›ä¿¡æ¯å§ã€‚ä»#tr[codepoint]`U+0041`å­—æ¯`A`å¼€å§‹ã€‚é¦–å…ˆï¼Œåœ¨`UnicodeData.txt`æ–‡ä»¶ä¸­æœ‰å¦‚ä¸‹æ•°æ®ï¼š

```
0041;LATIN CAPITAL LETTER A;Lu;0;L;;;;;N;;;;0061;
```

// After the codepoint and official name, we get the general category, which is `Lu`, or "Letter, uppercase." The next field, 0, is useful when you're combining and decomposing characters, which we'll look at later. The `L` tells us this is a strong left-to-right character, which is of critical importance when we look at bidirectionality in later chapters. Otherwise, `UnicodeData.txt` doesn't tell us much about this letter - it's not a character composed of multiple characters stuck together and it's not a number, so the next three fields are blank. The `N` means it's not a character that mirrors when the script flips from left to right (like parentheses do between English and Arabic. The next two fields are no longer used. The final fields are to do with upper and lower case versions: Latin has upper and lower cases, and this character is simple enough to have a single unambiguous lower case version, codepoint U+0061. It doesn't have upper or title case versions, because it already is upper case, duh.
åœ¨å¼€å¤´çš„#tr[codepoint]å’Œå®˜æ–¹#tr[character]åç§°ä¹‹åå°±æ˜¯#tr[general category]å±æ€§ï¼Œå…¶å€¼ä¸º `Lu`ï¼Œå«ä¹‰ä¸ºâ€œå­—æ¯ï¼ˆLetterï¼‰çš„å¤§å†™ï¼ˆuppercaseï¼‰å½¢å¼â€ã€‚ä¸‹ä¸€ä¸ªå±æ€§çš„å€¼æ˜¯ `0`ï¼Œè¿™ä¸ªå±æ€§æ˜¯ç”¨äºæŒ‡å¯¼å¦‚ä½•è¿›è¡Œ#tr[character]çš„#tr[combine]å’Œ#tr[decompose]çš„ï¼Œè¿™æ–¹é¢çš„å†…å®¹æˆ‘ä»¬åœ¨åé¢ä¼šä»‹ç»ï¼Œæš‚æ—¶å…ˆè·³è¿‡ã€‚ä¸‹ä¸€ä¸ªå±æ€§å€¼æ˜¯`L`ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬`A`æ˜¯ä¸€ä¸ªå¼ºçƒˆå€¾å‘äºä»å·¦å¾€å³ä¹¦å†™çš„#tr[character]ï¼Œè¿™ä¸€å±æ€§å¯¹äºåç»­ä¼šä»‹ç»çš„æ–‡æœ¬#tr[bidi]æ€§è‡³å…³é‡è¦ã€‚é™¤æ­¤ä¹‹å¤–`UnicodeData.txt`ä¸­çš„å…³äº`A`å°±æ²¡æœ‰å¤ªå¤šæœ‰ç”¨çš„ä¿¡æ¯äº†ï¼šå®ƒä¸æ˜¯ä¸€ä¸ªç”±å¤šä¸ª#tr[character]ç»„åˆè€Œæˆçš„#tr[character]ï¼Œä¹Ÿä¸æ˜¯æ•°å­—ï¼Œæ‰€ä»¥åé¢çš„å‡ ä¸ªå±æ€§éƒ½æ˜¯ç©ºçš„ã€‚ä¸‹ä¸€ä¸ªæœ‰å€¼çš„æ˜¯`N`ï¼Œå®ƒè¡¨ç¤ºå½“æ–‡æœ¬çš„ä¹¦å†™æ–¹å‘ç¿»è½¬æ—¶ï¼Œè¿™ä¸ªå­—ç¬¦ä¸éœ€è¦è¿›è¡Œé•œåƒã€‚å¯ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œåœ†æ‹¬å·åœ¨é˜¿æ‹‰ä¼¯æ–‡ç¯å¢ƒä¸­ï¼Œç›¸å¯¹äºåœ¨è‹±æ–‡ä¸­å°±æ˜¯éœ€è¦é•œåƒçš„ï¼Œä½†`A`ä¸éœ€è¦ã€‚æ¥ä¸‹æ¥çš„ä¸¤ä¸ªå±æ€§å·²ç»ä¸å†ä½¿ç”¨äº†ï¼Œç›´æ¥è·³è¿‡å®ƒä»¬ã€‚æœ€åä¸€ä¸ªå±æ€§åˆ™æ˜¯æœ‰å…³å¤§å°å†™è½¬æ¢çš„ã€‚æ‹‰ä¸å­—æ¯æœ‰å¤§å°å†™å½¢å¼ï¼Œè€Œ`A`#tr[character]æ¯”è¾ƒç®€å•ï¼Œå®ƒæ‹¥æœ‰ä¸€ä¸ªæ˜ç¡®çš„å°å†™ç‰ˆæœ¬ï¼Œç ä½ä¸º`U+0061`ã€‚å®ƒæ²¡æœ‰å¤§å†™æˆ–æ ‡é¢˜å½¢å¼ï¼Œå‘ƒï¼Œå› ä¸ºå®ƒæœ¬èº«å·²ç»æ˜¯å¤§å†™äº†å˜›ã€‚

// What else do we know about this character? Looking in `Blocks.txt` we can discover that it is part of the range, `0000..007F; Basic Latin`. `LineBreak.txt` is used by the line breaking algorithm, something we'll also look at in the chapter on layout.
å…³äºè¿™ä¸ª#tr[character]æˆ‘ä»¬è¿˜èƒ½çŸ¥é“äº›ä»€ä¹ˆå‘¢ï¼Ÿæ‰“å¼€ `Blocks.txt` æ–‡ä»¶ï¼Œæˆ‘ä»¬èƒ½çŸ¥é“å®ƒå¤„äº `0000..007F; Basic Latin` è¿™ä¸ªèŒƒå›´åŒºé—´ã€‚

`LineBreak.txt`ä¸­åˆ™æä¾›äº†ä¸€äº›åœ¨æ–­è¡Œç®—æ³•ä¸­éœ€è¦ç”¨åˆ°çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¼šåœ¨å…³äº#tr[layout]çš„ç« èŠ‚ä¸­è¯¦ç»†ä»‹ç»ï¼Œç°åœ¨å…ˆç®€å•çœ‹çœ‹ï¼š

#[
#set text(0.8em)

```
0041..005A;AL     # Lu    [26] LATIN CAPITAL LETTER A..LATIN CAPITAL LETTER Z
```
]


// This tells us that the upper case A is an alphabetic character for the purposes of line breaking. `PropList.txt` is a rag-tag collection of Unicode property information, and we will find two entries for our character there:
è¿™ä¸€è¡Œå‘Šè¯‰æˆ‘ä»¬ï¼Œå¯¹äºæ–­è¡Œæ¥è¯´ï¼Œå¤§å†™å­—æ¯ A å±äº `AL` ç±»åˆ«ï¼Œå«ä¹‰ä¸ºå­—æ¯å’Œå¸¸è§„ç¬¦å·ã€‚

`PropList.txt` æ˜¯ä¸€ä¸ªæ··æ‚äº†å„ç§Unicodeå±æ€§ä¿¡æ¯çš„é›†åˆï¼Œåœ¨å…¶ä¸­å’Œæˆ‘ä»¬çš„`A`#tr[character]æœ‰å…³çš„æœ‰ä¸¤æ¡ï¼š

#[
#set text(0.7em)

```
0041..0046    ; Hex_Digit # L&   [6] LATIN CAPITAL LETTER A..LATIN CAPITAL LETTER F
0041..0046    ; ASCII_Hex_Digit # L&   [6] LATIN CAPITAL LETTER A..LATIN CAPITAL LETTER F
```
]

// These tell us that it is able to be used as a hexadecimal digit, both in a more relaxed sense and strictly as a subset of ASCII. (U+FF21, a full-width version of `ï¼¡` used occasionally when writing Latin characters in Japanese text, is a hex digit, but it's not an ASCII hex digit.) `CaseFolding.txt` tells us:
è¿™äº›ä¿¡æ¯å‘Šè¯‰æˆ‘ä»¬ï¼Œ`A` èƒ½å¤Ÿç”¨åœ¨åå…­è¿›åˆ¶æ•°å­—ä¸­ï¼Œåœ¨å®½æ¾ç¯å¢ƒå’Œä¸¥æ ¼çš„ASCIIç¯å¢ƒä¸­å‡å¯ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œ`U+FF21` å…¨å®½çš„ `ï¼¡` åœ¨æ—¥æ–‡æ–‡æœ¬ä¸­ä½œä¸ºæ‹‰ä¸#tr[character]ä½¿ç”¨ã€‚å®ƒä¹Ÿå±äºåå…­è¿›åˆ¶æ•°å­—ï¼Œä½†ä¸å±äº ASCII åå…­è¿›åˆ¶æ•°å­—ã€‚

`CaseFolding.txt` ä¸­çš„ä¿¡æ¯å¦‚ä¸‹ï¼š

```
0041; C; 0061; # LATIN CAPITAL LETTER A
```

// When you want to case-fold a string containing `A`, you should replace it with codepoint `0061`, which as we've already seen, is LATIN SMALL LETTER A. Finally, in `Scripts.txt`, we discover...
è¿™è¡¨ç¤ºå½“ä½ å¸Œæœ›å¯¹ä¸€ä¸ªå«æœ‰`A`çš„å­—ç¬¦ä¸²è¿›è¡Œ#tr[case-fold]æ—¶ï¼Œéœ€è¦æŠŠå®ƒæ›¿æ¢ä¸º#tr[codepoint]`0061`ï¼Œä¹Ÿå°±æ˜¯`LATIN SMALL LETTER A`ã€‚

æœ€åï¼Œåœ¨`Scripts.txt`æ–‡ä»¶ä¸­ï¼Œå¯ä»¥å‘ç°ï¼š

#[
#set text(0.8em)

```
0041..005A    ; Latin # L&  [26] LATIN CAPITAL LETTER A..LATIN CAPITAL LETTER Z
```
]

// ..that this codepoint is part of the Latin script. You knew that, but now a computer does too.
è¿™ä¸€æ¡è¡¨ç¤ºè¿™ä¸ª#tr[codepoint]å±äºæ‹‰ä¸æ–‡ã€‚å½“ç„¶ï¼Œè¿™ä¸ªä¿¡æ¯ä½ æ—©å°±çŸ¥é“äº†ï¼Œä½†è®¡ç®—æœºæœ‰äº†è¿™ä¸ªæ–‡ä»¶æ‰èƒ½çŸ¥é“ã€‚

// Now let's look at a more interesting example. N'ko is a script used to write the Mandinka and Bambara languages of West Africa. But if we knew nothing about it, what could the Unicode Character Database teach us? Let's look at a sample letter, U+07DE NKO LETTER KA (ß).
ç°åœ¨æˆ‘ä»¬æ¢ä¸€ä¸ªæ›´æœ‰è¶£çš„ä¾‹å­å§ã€‚N'koï¼ˆä¹Ÿè¢«ç§°ä¸ºè¥¿éä¹¦é¢å­—æ¯æˆ–æ©ç§‘å­—æ¯ï¼‰æ˜¯ä¸€ç§ç”¨äºä¹¦å†™è¥¿éåœ°åŒºçš„æ›¼ä¸æˆˆè¯­å’Œç­å·´æ‹‰è¯­çš„#tr[script]ã€‚å¦‚æœæˆ‘ä»¬å¯¹è¿™ç§#tr[script]ä¸€æ— æ‰€çŸ¥çš„è¯ï¼ŒUnicode#tr[character]æ•°æ®åº“èƒ½å¤Ÿæä¾›ç»™æˆ‘ä»¬ä¸€äº›ä»€ä¹ˆä¿¡æ¯å‘¢ï¼Ÿæˆ‘ä»¬ç”¨`U+07DE NKO Letter KA`ï¼ˆ#mandingo[ß]ï¼‰è¿™ä¸ªå­—æ¯æ¥è¯•è¯•å§ã€‚

// First, from `UnicodeData.txt`:
é¦–å…ˆï¼Œ`UnicodeData.txt`ä¸­çš„ä¿¡æ¯æœ‰ï¼š

```
07DE;NKO LETTER KA;Lo;0;R;;;;;N;;;;;
```

// This tells me that the character is a Letter-other - neither upper nor lower case - and the lack of case conversion information at the end confirms that N'ko is a unicase script. The `R` tells me that N'ko is written from right to left, like Hebrew and Arabic. It's an alphabetic character for line breaking purposes, according to `LineBreak.txt`, and there's no reference to it in `PropList.txt`. But when we look in `ArabicShaping.txt` we find something very interesting:
è¿™è¡Œå‘Šè¯‰æˆ‘ä»¬ï¼Œè¿™ä¸ª#tr[character]å±äº`Lo`#tr[general category]ï¼Œè¡¨ç¤ºå®ƒæ˜¯ä¸€ä¸ªâ€œå­—æ¯ï¼ˆLetterï¼‰çš„å…¶ä»–ï¼ˆotherï¼‰å½¢å¼â€ã€‚è¿™é‡Œçš„å…¶ä»–å½¢å¼è¡¨ç¤ºæ—¢ä¸æ˜¯å¤§å†™ä¹Ÿä¸æ˜¯å°å†™ã€‚è¿™è¡Œæœ€åç¼ºå°‘çš„å¤§å°å†™è½¬æ¢ä¿¡æ¯ä¹Ÿå‘Šè¯‰æˆ‘ä»¬ï¼ŒN'ko æ˜¯ä¸€ç§ä¸åˆ†å¤§å°å†™çš„#tr[script]ã€‚ä¸­é—´çš„`R`åˆ™è¡¨ç¤ºN'ko#tr[script]æ˜¯ä»å³å¾€å·¦ä¹¦å†™çš„ï¼Œå°±åƒå¸Œä¼¯æ¥æ–‡å’Œé˜¿æ‹‰ä¼¯æ–‡é‚£æ ·ã€‚

æ ¹æ® `LineBreak.txt` ä¸­çš„ä¿¡æ¯ï¼Œå¯¹äºæ–­è¡Œæ¥è¯´å®ƒå’Œ`A`çš„ç±»åˆ«ä¸€æ ·ï¼Œä¹Ÿå±äºæ™®é€šå­—æ¯è¿™ä¸€ç±»ã€‚åœ¨ `PropList.txt` ä¸­æ²¡æœ‰å…³äºå®ƒçš„ä¿¡æ¯ã€‚ä½†å½“æˆ‘ä»¬æ‰“å¼€ `ArabicShaping.txt` è¿™ä¸ªæ–‡ä»¶æ—¶ï¼Œä¼šå‘ç°ä¸€äº›æœ‰è¶£çš„ä¿¡æ¯ï¼š

```
07DE; NKO KA; D; No_Joining_Group
```

// The letter ß is a double-joining character, meaning that N'ko is a connected script like Arabic, and the letter Ka connects on both sides. That is, in the middle of a word like "n'ko" ("I say"), the letter looks like this: ß’ßß.
è¿™ä¸ªå­—æ¯#mandingo[ß]æ˜¯ä¸€ä¸ªåŒå‘äº’è¿#tr[character]ï¼Œè¿™æ„å‘³ç€N'koæ˜¯ä¸€ç§å’Œé˜¿æ‹‰ä¼¯æ–‡ç±»ä¼¼çš„è¿å†™#tr[script]ï¼Œå¹¶ä¸” Ka å­—æ¯å’Œå·¦å³ä¸¤è¾¹éƒ½äº’ç›¸è¿æ¥ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨è¡¨ç¤ºâ€œæˆ‘è¯´â€çš„å•è¯â€œn'koâ€é‡Œï¼Œè¿™ä¸ªå­—æ¯çœ‹ä¸Šå»æ˜¯è¿™æ ·çš„ï¼š#mandingo[ß’ßß]ã€‚

// This is the kind of data that text processing systems can derive programmatically from the Unicode Character Database. Of course, if you really want to know about how to handle N'ko text and the N'ko writing system in general, the Unicode Standard itself is a good reference point: its section on N'ko (section 19.4) tells you about the origins of the script, the structure, diacritical system, punctuation, number systems and so on.
è¿™å°±æ˜¯æ–‡å­—å¤„ç†ç³»ç»Ÿèƒ½ç”¨ç¨‹åºä»Unicode#tr[character]æ•°æ®åº“ä¸­è·å–åˆ°çš„ä¿¡æ¯ã€‚å½“ç„¶ï¼Œå¦‚æœä½ çœŸçš„æƒ³å­¦ä¹ N'ko#tr[writing system]çš„é€šç”¨çŸ¥è¯†å’Œå¦‚ä½•å¤„ç†å®ƒä»¬ï¼ŒUnicodeæ ‡å‡†æœ¬èº«å°±æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å‚è€ƒèµ„æ–™ï¼Œå®ƒçš„ N'ko éƒ¨åˆ†ï¼ˆåœ¨ 19.4 å°èŠ‚ï¼‰ä¼šå‘Šè¯‰ä½ è¿™ç§#tr[script]çš„èµ·æºã€ç»“æ„ã€#tr[diacritic]ã€æ ‡ç‚¹ã€æ•°å­—ç³»ç»Ÿç­‰æ–¹é¢çš„ä¿¡æ¯ã€‚

#note[
  // > When dealing with computer processing of an unfamiliar writing system, the Unicode Standard is often a good place to start. It's actually pretty readable and contains a wealth of information about script issues. Indeed, if you're doing any kind of heavy cross-script work, you would almost certainly benefit from getting hold of a (printed) copy of the latest Unicode Standard as a desk reference.
  å½“ä½¿ç”¨è®¡ç®—æœºå¤„ç†ä¸€ç§ä¸ç†Ÿæ‚‰çš„#tr[writing system]æ—¶ï¼ŒUnicodeæ ‡å‡†é€šå¸¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚å®ƒçš„æ˜“è¯»æ€§å…¶å®éå¸¸å¼ºï¼Œè€Œä¸”åŒ…å«åœ¨å¤„ç†#tr[script]æ—¶ä¼šé‡åˆ°çš„å„ç§é—®é¢˜çš„å…³é”®ä¿¡æ¯ã€‚äº‹å®ä¸Šï¼Œåœ¨è¿›è¡Œä»»ä½•ç¹é‡çš„è·¨#tr[scripts]å·¥ä½œæ—¶ï¼Œå¦‚æœåœ¨æ‰‹è¾¹èƒ½æœ‰ä¸€æœ¬ï¼ˆçº¸è´¨ç‰ˆçš„ï¼‰æœ€æ–°ç‰ˆUnicodeæ ‡å‡†æ–‡æ¡£ä½œä¸ºå‚è€ƒæ‰‹å†Œï¼Œå¿…ç„¶ä¼šå¤§æœ‰è£¨ç›Šã€‚
]

// ## Case conversion
== å¤§å°å†™è½¬æ¢

// We've seen that N'Ko is a unicase script; its letters have no upper case and lower case forms. In fact, only a tiny minority of writing systems, such as those based on the Latin alphabet, have the concept of upper and lower case versions of a character. For some language systems like English, this is fairly simple and unambiguous. Each of the 26 letters of the Latin alphabet used in English have a single upper and lower case. However, other languages which use cases often have characters which do not have such a simple mapping. The Unicode character database, and especially the file `SpecialCasing.txt`, provides machine-readable information about case conversion.
æˆ‘ä»¬å·²ç»çŸ¥é“ï¼ŒN'ko æ˜¯ä¸€ç§ä¸åˆ†å¤§å°å†™çš„#tr[script]ã€‚äº‹å®ä¸Šåªæœ‰å¾ˆå°ä¸€éƒ¨åˆ†ä¹¦å†™ç³»ç»Ÿæ‹¥æœ‰å¤§å°å†™çš„æ¦‚å¿µï¼Œæ¯”å¦‚é‚£äº›åŸºäºæ‹‰ä¸å­—æ¯è¡¨çš„#tr[scripts]ã€‚å¯¹äºç±»ä¼¼è‹±è¯­çš„ä¸€äº›è¯­è¨€æ¥è¯´ï¼Œå¤§å°å†™çš„æ¦‚å¿µå¾ˆæ¸…æ™°ï¼šè‹±è¯­ä¸­çš„26ä¸ªæ‹‰ä¸å­—æ¯å„è‡ªéƒ½åªæœ‰ä¸€ä¸ªå¤§å†™å½¢å¼å’Œä¸€ä¸ªå°å†™å½¢å¼ã€‚ä½†æ˜¯å…¶ä»–çš„è¯­è¨€ä¸ä¸€å®šä¹Ÿéµå¾ªè¿™æ ·ç®€å•çš„æ˜ å°„ã€‚Unicode#tr[character]æ•°æ®åº“ä¸­çš„ `SpecialCasing.txt` æ–‡ä»¶ä»¥æœºå™¨å¯è¯»çš„æ ¼å¼è¡¨è¿°äº†å…³äºå¤§å°å†™è½¬æ¢çš„ä¿¡æ¯ã€‚

// The classic example is German. When the sharp-s character U+00DF (ÃŸ) is uppercased, it becomes the *two* characters "SS". There is clearly a round-tripping problem here, because when the characters "SS" are downcased, they become "ss", not ÃŸ. For more fun, Unicode also defines the character U+1E9E, LATIN CAPITAL LETTER SHARP S (áº), which downcases to ÃŸ.
ä¸€ä¸ªéå¸¸ç»å…¸çš„ä¾‹å­æ˜¯ï¼Œå¾·è¯­ä¸­æœ‰ä¸€ä¸ªè¢«ç§°ä¸ºsharp sçš„å­—æ¯#german[ÃŸ]ï¼Œä½äº `U+00DF`ã€‚å½“è¿›è¡Œå¤§å†™è½¬æ¢æ—¶ï¼Œéœ€è¦å°†å®ƒå˜æˆ*ä¸¤ä¸ª*#tr[character]SSã€‚è¿™é‡Œæ˜¾ç„¶ä¼šå­˜åœ¨ä¸€ä¸ªé€†å‘è½¬æ¢çš„é—®é¢˜ï¼Œå› ä¸ºå½“SSè½¬æ¢ä¸ºå°å†™æ—¶ä¼šæ˜¯ssï¼Œè€Œä¸å†æ˜¯#german[ÃŸ]ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼ŒUnicodeä¹Ÿå®šä¹‰äº†`U+1E9E LATIN CAPITAL LETTER SHARP S`ï¼ˆæ‹‰ä¸æ–‡å¤§å†™å­—æ¯ Sharp Sï¼‰ï¼Œå†™ä½œ #german[áº]ã€‚è¿™ä¸ª#tr[character]è½¬æ¢æˆå°å†™æ˜¯#german[ÃŸ]ã€‚

#note[
  // > During the writing of this book, the Council for German Orthography (*Rat fÃ¼r deutsche Rechtschreibung*) has recommended that the LATIN CAPITAL LETTER SHARP S be included in the German alphabet as the uppercase version of ÃŸ, which will make everything a lot easier but rob us of a very useful example of the difficulties of case conversion.
  åœ¨æœ¬ä¹¦çš„ç¼–å†™è¿‡ç¨‹ä¸­ï¼Œå¾·è¯­æ­£å†™æ³•åä¼šï¼ˆ#german[Rat fÃ¼r deutsche Rechtschreibung]ï¼‰å»ºè®®å°†`LATIN CAPITAL LETTER SHARP S`ä½œä¸º#german[ÃŸ]çš„å¤§å†™å½¢å¼çº³å…¥å¾·æ–‡å­—æ¯è¡¨ä¸­ã€‚è¿™ä¼šä½¿ä¸Šè¿°éš¾é¢˜å˜å¾—ç®€å•å¾ˆå¤šï¼Œä½†è¿™æ ·ä¹Ÿä¼šè®©æˆ‘ä»¬å¤±å»ä¸€ä¸ªèƒ½ç›´è§‚æ„Ÿå—åˆ°å¤§å°å†™è½¬æ¢çš„å›°éš¾ç¨‹åº¦çš„ç»ä½³ä¾‹å­ã€‚
]

// The other classic example is Turkish. The ordinary Latin small letter "i" (U+0069) normally uppercases to "I" (U+0049) - except when the document is written in Turkish or Azerbaijani, when it uppercases to "Ä°". This is because there is another letter used in those languages, LATIN SMALL LETTER DOTLESS I (U+0131, Ä±), which uppercases to "I". So case conversion needs to be aware of the linguistic background of the text.
å¦ä¸€ä¸ªç»å…¸ä¾‹å­æ˜¯ä¼ ç»Ÿæ‹‰ä¸å­—æ¯iï¼ˆ`U+0069`ï¼‰ã€‚é€šå¸¸æ¥è¯´ï¼Œè¿™ä¸ªå­—æ¯è½¬æ¢ä¸ºå¤§å†™æ—¶ä¼šæ˜¯Iï¼ˆU+0049ï¼‰ï¼Œä½†åœ¨åœŸè€³å…¶è¯­æˆ–é˜¿å¡æ‹œç–†è¯­çš„ç¯å¢ƒä¸‹ï¼Œå…¶å¤§å†™å´æ˜¯Ä°ã€‚<position:turkish-i-uppercase>è¿™æ˜¯å› ä¸ºè¿™äº›è¯­è¨€ä¸­è¿˜æœ‰å¦ä¸€ä¸ªå­—æ¯ Ä±ï¼ˆ`U+0131 LATIN SMALL LETTER DOTLESS I`ï¼Œæ‹‰ä¸æ–‡å°å†™å­—æ¯æ— ç‚¹Iï¼‰ï¼Œè¿™ä¸ªå­—æ¯çš„å¤§å†™æ‰æ˜¯ Iã€‚æ‰€ä»¥ï¼Œå¤§å°å†™è½¬æ¢ä¹Ÿéœ€è¦è€ƒè™‘æ–‡æœ¬æ‰€å¤„çš„è¯­è¨€ç¯å¢ƒã€‚

// As well as depending on language, case conversion also depends on context. GREEK CAPITAL LETTER SIGMA (Î£) downcases to GREEK SMALL LETTER SIGMA (Ïƒ) except at the end of a word, in which case it downcases to Ï‚, GREEK SMALL LETTER FINAL SIGMA.
é™¤äº†å’Œè¯­è¨€æœ‰å…³ä¹‹å¤–ï¼Œå¤§å°å†™è½¬æ¢è¿˜éœ€è¦ç»“åˆä¸Šä¸‹æ–‡ã€‚#greek[Î£]ï¼ˆ`GREEK CAPITAL LETTER SIGMA`ï¼Œå¸Œè…Šæ–‡å¤§å†™å­—æ¯ Sigmaï¼‰é€šå¸¸çš„å°å†™å½¢å¼ä¸º#greek[Ïƒ]ï¼ˆ`GREEK SMALL LETTER SIGMA`ï¼Œå¸Œè…Šæ–‡å°å†™å­—æ¯ Sigmaï¼‰ã€‚ä½†å¦‚æœå…¶å‡ºç°åœ¨è¯å°¾ï¼Œåˆ™å°å†™å½¢å¼ä¼šå˜ä¸º#greek[Ï‚]ï¼ˆ`GREEK SMALL LETTER FINAL SIGMA`ï¼Œå¸Œè…Šæ–‡å°å†™å­—æ¯è¯å°¾ Sigmaï¼‰ã€‚

// Another example comes from the fact that Unicode may have a *composed form* for one case, but not for another. Code point U+0390 in Unicode is occupied by GREEK SMALL LETTER IOTA WITH DIALYTIKA AND TONOS, which looks like this: Î. But for whatever reason, Unicode never encoded a corresponding GREEK CAPITAL LETTER IOTA WITH DIALYTIKA AND TONOS. Instead, when this is placed into upper case, three code points are required: U+0399, GREEK CAPITAL LETTER IOTA provides the Î™; then U+0308 COMBINING DIAERESIS provides the dialytika; and lastly, U+0301 COMBINING ACUTE ACCENT provides the tonos.
è¿˜æœ‰ä¸€ç§æƒ…å†µæ˜¯ï¼ŒæŸäº›å­—ç¬¦çš„ä¸€ç§å½¢æ€çš„æ˜¯*#tr[compose]å½¢å¼*çš„ï¼Œä½†å¦ä¸€ç§å´ä¸æ˜¯ã€‚æ¯”å¦‚å­—æ¯#greek[Î]ï¼ˆ`U+0390 GREEK SMALL LETTER IOTA WITH DIALYTIKA AND TONOS`ï¼Œå¸¦Dialytikaå’ŒTonosçš„å¸Œè…Šæ–‡å°å†™å­—æ¯ Iotaï¼‰ï¼ŒUnicodeä¸­æ²¡æœ‰æŸä¸ªå•ä¸€#tr[character]å¯¹åº”çš„å®ƒçš„å¤§å†™å½¢æ€ã€‚å½“æˆ‘ä»¬éœ€è¦å®ƒçš„å¤§å†™æ—¶ï¼Œå¾—ç”¨ä¸Šä¸‰ä¸ª#tr[codepoint]ï¼š`U+0399 GREEK CAPITAL LETTER IOTA`ï¼ˆå¸Œè…Šæ–‡å¤§å†™å­—æ¯ Iotaï¼‰æä¾›ä¸»ä½“ï¼›`U+0308 COMBINING DIAERESIS`ï¼ˆç»„åˆç”¨åˆ†éŸ³ç¬¦ï¼‰æä¾›åˆ†éŸ³ç¬¦ï¼ˆå­—æ¯ä¸Šæ–¹çš„ä¸¤ä¸ªç‚¹ï¼‰ï¼›`U+0301 COMBINING ACUTE ACCENT`ï¼ˆç»„åˆç”¨é”éŸ³ç¬¦ï¼‰æä¾›å£°è°ƒã€‚å®ƒä»¬åˆèµ·æ¥æ‰èƒ½ç»„æˆå¤§å†™çš„#greek[\u{0399}\u{0308}\u{0301}/* ÎªÌ */]ã€‚

// ## Normalization and decomposition
== #tr[normalization]å’Œ#tr[decompose] <heading:normalization-decomposition>

// The Unicode Standard has a number of stated design goals: to be *universal*, in the sense that every character in every script likely to be used on computer has an encoding; to be *efficient*, such that algorithms used by computers to input and output Unicode characters do not require too much state or overhead; and to be *unambiguous*, in that every Unicode codepoint represents the same character.
Unicodeæ ‡å‡†æ˜é¢ä¸Šæœ‰å¦‚ä¸‹å‡ ä¸ªè®¾è®¡ç›®æ ‡ï¼šé€šç”¨ï¼Œä¹Ÿå³å°½é‡#tr[encoding]æ‰€æœ‰å¯èƒ½åœ¨è®¡ç®—æœºä¸­ç”¨åˆ°çš„æ‰€æœ‰#tr[scripts]ä¸­çš„æ‰€æœ‰#tr[character]ï¼›é«˜æ•ˆï¼Œå¤„ç†Unicodeæ•°æ®çš„è¾“å…¥è¾“å‡ºçš„ç›¸å…³ç®—æ³•ä¸èƒ½å¼•å…¥è¿‡å¤šçš„é¢å¤–çŠ¶æ€å’Œè®¡ç®—å¼€é”€ï¼›æ˜ç¡®ä¸”æ— æ­§ä¹‰ï¼ŒUnicode#tr[codepoint]å’Œ#tr[character]äº’ç›¸ä¸€ä¸€å¯¹åº”ã€‚

// But it also has an unstated design goal, which is the unstated design goal of pretty much every well-behaved piece of software engineering: *backward compatibility*. Unicode wants to maintain compatibility with all previous encoding standards, so that old documents can be reliably converted to and from Unicode without ambiguity. To a first approximation, this means every character that was ever assigned a codepoint in some encoding should be assigned a unique codepoint in Unicode.
é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ªæ²¡æœ‰ç›´æ¥è¡¨è¿°å‡ºï¼Œä½†å’Œç»å¤§å¤šæ•°è½¯ä»¶å·¥ç¨‹é¡¹ç›®ä¸€æ ·å¿…é¡»è€ƒè™‘åˆ°çš„ç›®æ ‡ï¼š*å‘åå…¼å®¹*ã€‚Unicodeå¸Œæœ›å°½é‡å¯¹å…ˆå‰çš„#tr[encoding]æ ‡å‡†ä¿æŒå…¼å®¹ï¼Œè¿™æ ·è€æ—§çš„æ–‡æ¡£æ‰èƒ½è¢«å¯é åœ°åœ¨åŸå§‹#tr[encoding]å’ŒUnicodeé—´æ¥å›è½¬æ¢ã€‚è¿™å‡ ä¹å°±æ˜¯è¯´ï¼Œåªè¦å…ˆå‰çš„æŸä¸ª#tr[encoding]æ ‡å‡†ä¸­æœ‰æŸä¸ª#tr[character]ï¼ŒUnicodeä¸­å°±å¿…é¡»ä¸ºå®ƒèµ‹äºˆä¸€ä¸ªä¸“å±çš„#tr[codepoint]ã€‚

// This contrasts somewhat with the goal of unambiguity, as there can be multiple different ways to form a character. For instance, consider the  character nÌˆ (Latin small letter n with diaeresis). It occurs in Jacaltec, Malagasy, Cape Verdean Creole, and most notably, *This Is SpÄ±nÌˆal Tap*. Despite this obvious prominence, it is not considered noteworthy enough to be encoded in Unicode as a distinct character, and so has to be encoded using *combining characters*.
è¿™åœ¨æŸç§ç¨‹åº¦ä¸Šå’Œä¹‹å‰è¯´çš„æ— æ­§ä¹‰ç›®æ ‡æ˜¯ç›¸è¿èƒŒçš„ã€‚å› ä¸ºåŒä¸€ä¸ª#tr[character]åœ¨ä¸åŒçš„å¤è€æ ‡å‡†ä¸­å¯èƒ½æœ‰ä¸åŒçš„æ„å»ºæ–¹å¼ã€‚æ¯”å¦‚#tr[character]nÌˆï¼ˆå¸¦æœ‰åˆ†éŸ³ç¬¦çš„å°å†™æ‹‰ä¸å­—æ¯Nï¼‰ï¼Œå®ƒåœ¨é›…å¡å°”æ³°å…‹è¯­ã€é©¬è¾¾åŠ æ–¯åŠ è¯­ã€å¡å¸ƒä½›å¾—é²è¯­å’Œè‘—åçš„ç”µå½±ã€ŠThis Is SpÄ±nÌˆal Tapã€‹ä¸­éƒ½æœ‰ä½¿ç”¨ã€‚å°½ç®¡å¦‚æ­¤å¸¸è§ï¼Œä½†Unicodeä¸è®¤ä¸ºå®ƒé‡è¦åˆ°è¦ç”¨ä¸€ä¸ªå•ç‹¬çš„#tr[character]æ¥#tr[encoding]ã€‚æœ€ç»ˆæˆ‘ä»¬ç”¨ä¸€ç§å«åš*å¯#tr[combine]#tr[character]*çš„æ–¹å¼#tr[encoding]å®ƒã€‚

// A combining character is a mark that attaches to a base character; in other words, a diacritic. To encode nÌˆ, we take LATIN SMALL LETTER N (U+006E) and follow it with COMBINING DIAERESIS (U+0308). The layout system is responsible for arranging for those two characters to be displayed as one.
å¯#tr[combine]#tr[character]æ˜¯ä¸€ç§é™„åŠ åˆ°åŸºæœ¬#tr[character]ä¸Šçš„ç¬¦å·ï¼Œä¹Ÿè¢«ç§°ä¸º#tr[diacritic]ã€‚ä¸ºäº†#tr[encoding]nÌˆï¼Œé¦–å…ˆéœ€è¦#tr[character]`U+006E LATIN SMALL LETTER N`ï¼ˆæ‹‰ä¸æ–‡å°å†™å­—æ¯Nï¼‰ï¼Œç„¶ååœ¨å…¶ååŠ ä¸Š#tr[character]`U+0308 COMBINING DIAERESIS`ï¼ˆ#tr[combine]ç”¨åˆ†éŸ³ç¬¦ï¼‰ã€‚#tr[layout]ç³»ç»Ÿä¼šè´Ÿè´£å°†è¿™ä¸¤ä¸ª#tr[character]ç¼–æ’æ˜¾ç¤ºä¸ºä¸€ä¸ªæ•´ä½“\u{006E}\u{0308}ã€‚

#note[
  // > This obviously walks all over the "efficiency" design goal - applications which process text character-by-character must now be aware that something which visually looks like one character on the page, and semantically refers to one character in text, is actually made up of *two* characters in a string, purely as an artifact of Unicode's encoding rules. Poorly-written software can end up separating the two characters and processing them independently, instead of treating them as one indivisible entity.
  è¿™æ˜¾ç„¶è¿èƒŒäº†å…³äºé«˜æ•ˆçš„è®¾è®¡ç›®æ ‡ã€‚æ–‡æœ¬å¤„ç†åº”ç”¨å¿…é¡»èƒ½å¤Ÿç†è§£ï¼Œæœ‰äº›åœ¨é¡µé¢ä¸Šçœ‹ä¸Šå»æ˜¯ä¸€ä¸ª#tr[character]ï¼Œåœ¨æ–‡æœ¬æ¦‚å¿µä¸­ä¹Ÿæ˜¯ä¸€ä¸ª#tr[character]çš„ä¸œè¥¿ï¼Œåˆ°äº†ç¼–ç¨‹é¢†åŸŸçš„å­—ç¬¦ä¸²æ¦‚å¿µä¸­å´æ˜¯*ä¸¤ä¸ª*#tr[character]ã€‚è€Œè¿™çº¯ç²¹åªæ˜¯Unicode#tr[encoding]è§„åˆ™çš„äº§ç‰©ã€‚è´¨é‡ä¸€èˆ¬çš„è½¯ä»¶å¯èƒ½ä¼šæŠŠè¿™ä¸¤ä¸ª#tr[character]åˆ†å¼€å¹¶å•ç‹¬å¤„ç†ï¼Œè€Œä¸æ˜¯å°†å…¶è§†ä¸ºä¸å¯åˆ†å‰²çš„æ•´ä½“ã€‚
]

// Now consider the character á¹… (Latin small letter n with dot above). Just one dot different, but a different story entirely; this is used in the transliteration of Sanskrit, and as such was included in pre-Unicode encodings such as CS/CSX (Wujastyk, D., 1990, *Standardization of Sanskrit for Electronic Data Transfer and Screen Representation*, 8th World Sanskrit Conference, Vienna), where it was assigned codepoint 239. Many electronic versions of Sanskrit texts were prepared using the character, and so when it came to encoding it in Unicode, the backward compatibility goal meant that it needed to be encoded as a separate character, U+1E45.
ç°åœ¨æ¥çœ‹å¦ä¸€ä¸ª#tr[character]á¹…ï¼ˆä¸Šé¢å¸¦ç‚¹çš„æ‹‰ä¸æ–‡å°å†™å­—æ¯Nï¼‰ï¼Œå®ƒå’ŒnÌˆçœ‹èµ·æ¥åªæœ‰â€œä¸€ç‚¹â€åŒºåˆ«ï¼Œä½†åç»­çš„å‘½è¿å´è¿¥ç„¶ä¸åŒã€‚è¿™ä¸ªå­—æ¯ç”¨äºæ¢µæ–‡çš„æ‹‰ä¸è½¬å†™ï¼Œæ›¾ç»è¢«CS/CSX#tr[encoding]#footnote[1990å¹´ï¼Œç¬¬å…«å±Šä¸–ç•Œæ¢µè¯­å¤§ä¼šäºç»´ä¹Ÿçº³ä¸¾è¡Œã€‚åœ¨ä¼šè®®çš„ä¸€æ¬¡å…³äºæ¢µæ–‡åœ¨ç”µå­æ•°æ®ä¼ è¾“ä¸­çš„æ ‡å‡†åŒ–é—®é¢˜çš„å°ç»„è®¨è®ºä¼šä¸Šï¼Œ#cite(form: "prose", <Wujastyk.StandardizationSanskrit.1990>)æå‡ºäº†æ­¤ç¼–ç ã€‚]æ”¶å½•äº#tr[codepoint]239ä¸Šï¼Œè®¸å¤šä½¿ç”¨æ¢µæ–‡çš„ç”µå­æ–‡æœ¬éƒ½ä¼šä½¿ç”¨è¿™ä¸€#tr[character]ã€‚å› æ­¤ä¸ºäº†å‘åå…¼å®¹æ€§ï¼ŒUnicodeéœ€è¦å°†å®ƒè§†ä¸ºä¸€ä¸ªå•ç‹¬çš„#tr[character]ã€‚æœ€ç»ˆå®ƒè¢«æ”¾ç½®åœ¨#tr[codepoint]`U+1E45`ä¸Šã€‚

// But of course, it could equally be represented in just the same way as nÌˆ: you can form a á¹… by following a LATIN SMALL LETTER N (U+006E) with a COMBINING DOT ABOVE (U+0307). Two possible ways to encode á¹…, but only one possible way to encode nÌˆ. So much for "unambiguous": the two strings "U+006E U+0307" and "U+1E45" represent the same character, but are not equal.
ä½†æ˜¯å®ƒä¹Ÿç†æ‰€å½“ç„¶åœ°å¯ä»¥ç”¨#tr[encoding]nÌˆçš„é‚£ç§æ–¹å¼æ¥è¡¨ç¤ºï¼š`U+006E LATIN SMALL LETTER N`ï¼ˆæ‹‰ä¸æ–‡å°å†™å­—æ¯Nï¼‰å’Œ`U+0307 COMBINING DOT ABOVE`ï¼ˆ#tr[combine]ç”¨ä¸Šç‚¹ï¼‰è¿™ä¸¤ä¸ªå­—ç¬¦#tr[combine]èµ·æ¥ä¹Ÿæ˜¯\u{006E}\u{0307}ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰ä¸¤ç§ä¸åŒçš„#tr[encoding]å¯ä»¥è¡¨ç¤ºá¹…ï¼Œä½†åªæœ‰ä¸€ç§æ–¹å¼è¡¨ç¤ºnÌˆã€‚å¦å¤–ï¼Œ`U+006E U+0307`å’Œ`U+1E45`è¿™ä¸¤ä¸ªå­—ç¬¦ä¸²è¡¨ç¤ºåŒä¸€ä¸ª#tr[character]ï¼Œä½†å®ƒä»¬æ˜æ˜¾ä¸ç›¸ç­‰ã€‚çœ‹èµ·æ¥æ­§ä¹‰é—®é¢˜å¥½åƒè¶Šæ¥è¶Šä¸¥é‡äº†ã€‚

// But wait - and you're going to hear this a lot when it comes to Unicode - it gets worse! The sign for an Ohm, the unit of electrical resistance, is â„¦ (U+2126 OHM SIGN). Now while a fundamental principle of Unicode is that *characters encode semantics, not visual representation*, this is clearly in some sense "the same as" Î©. (U+03A9 GREEK CAPITAL LETTER OMEGA) They are semantically different but they happen to look the same; and yet, let's face it, from a user perspective it would be exceptionally frustrating if you searched in a string for a â„¦ but you didn't find it because the string contained a Î© instead.
åˆ«æ€¥ï¼Œè¿˜æœ‰æ›´ç³Ÿçš„ã€‚è¿˜è®°å¾—æˆ‘ä»¬è¯´è¿‡ï¼ŒUnicodeæ˜¯â€œæŒ‰ç…§è¯­ä¹‰ï¼Œè€Œä¸æ˜¯å¤–å½¢â€æ”¶å½•#tr[character]çš„ã€‚ä¸¾ä¾‹æ¥è¯´ï¼Œç”¨äºè¡¨ç¤ºç”µé˜»å•ä½çš„ â„¦ï¼ˆ`U+2126 OHM SIGN`ï¼Œæ¬§å§†æ ‡è®°ï¼‰å’ŒÎ©ï¼ˆ`U+03A9 GREEK CAPITAL LETTER OMEGA`ï¼Œå¸Œè…Šæ–‡å¤§å†™å­—æ¯Omegaï¼‰å°±å› è¯­ä¹‰ä¸åŒè€Œè¢«åˆ†åˆ«æ”¶å½•ã€‚ä½†æ˜¯ï¼Œè®©æˆ‘ä»¬é¢å¯¹ç°å®å§ã€‚å¦‚æœåœ¨ä¸€ä¸²æ–‡æœ¬ä¸­æœç´¢â„¦ï¼Œå´å› ä¸ºåœ¨å…¶ä¸­çš„æ˜¯Î©è€Œæ— æ³•æœç´¢å‡ºæ¥ï¼Œç«™åœ¨æ™®é€šç”¨æˆ·çš„è§’åº¦çœ‹è¿™ä¹Ÿå¤ªè¿·æƒ‘å’Œè’è°¬äº†ã€‚

// The way that Unicode deals with both of these problem is to define one of the encodings to be *canonical*. The Standard also defines two operations: *Canonical Decomposition* and *Canonical Composition*. Replacing each character in a string with its canonical form is called *normalization*.
Unicodeå¤„ç†ä¸Šè¿°é—®é¢˜çš„æ–¹å¼æ˜¯å®šä¹‰ä¸€ä¸ª#tr[canonical]#tr[encoding]ã€‚æ ‡å‡†ä¸­è¿˜å®šä¹‰äº†ä¸¤ä¸ªæ“ä½œï¼š*#tr[canonical]#tr[decompose]*å’Œ*#tr[canonical]#tr[compose]*ã€‚å°†å­—ç¬¦ä¸²ä¸­çš„æ‰€æœ‰#tr[character]è½¬åŒ–ä¸ºå…¶å¯¹åº”çš„#tr[canonical]å½¢å¼çš„è¿‡ç¨‹ç§°ä¸º*#tr[normalization]*ã€‚

#note[
  // > There's also a "compatibility decomposition", for characters which are very similar but not precisely equivalent: â„ (U+210D DOUBLE-STRUCK CAPITAL H) can be simplified to a Latin capital letter H. But the compatibility normalizations are rarely used, so we won't go into them here.
  å¦å¤–è¿˜æœ‰ä¸€ç§æ“ä½œå«å…¼å®¹#tr[decompose]ï¼Œå¯¹äºä¸€äº›éå¸¸ç›¸ä¼¼ï¼Œä½†å…¶å®æœ‰ç»†å¾®å·®åˆ«çš„#tr[character]è¿›è¡Œå…¼å®¹å¤„ç†ã€‚æ¯”å¦‚ â„ï¼ˆ`U+210D DOUBLE-STRUCK CAPITAL H`ï¼ŒåŒçº¿å¤§å†™Hï¼‰ï¼Œåœ¨å…¼å®¹#tr[decompose]æ“ä½œä¸‹ä¼šç®€åŒ–ä¸ºæ‹‰ä¸å¤§å†™å­—æ¯Hã€‚ä½†å› ä¸ºè¿™ç§æ–¹å¼å…¶å®å¾ˆå°‘ç”¨åˆ°ï¼Œæˆ‘ä»¬åœ¨æ­¤ä¸è¿›è¡Œè¿‡å¤šä»‹ç»ã€‚
]

// The simplest way of doing normalization is called Normalization Form D, or NFD. This just applies canonical decomposition, which means that every character which can be broken up into separate components gets broken up. As usual, the Unicode Database has all the information about how to decompose characters.
è¿›è¡Œ#tr[normalization]çš„æœ€ç®€å•çš„æ–¹æ³•æ˜¯NFDï¼ˆNormalization Form Dï¼Œ#tr[normalization]å½¢å¼Dï¼‰ï¼Œåªéœ€è¦æ‰§è¡Œä¸€é#tr[canonical]#tr[decompose]å³å¯ã€‚åœ¨NFDå®Œæˆåï¼Œæ¯ä¸ª#tr[character]éƒ½è¢«åˆ†è§£ä¸ºå…¶ç»„æˆéƒ¨åˆ†ã€‚å’Œä¹‹å‰ä¸€æ ·ï¼ŒUnicodeæ•°æ®åº“ä¹ŸåŒ…å«äº†å…³äºå¦‚ä½•#tr[decompose]#tr[character]çš„ä¿¡æ¯ã€‚

// Let's take up our example again of GREEK CAPITAL LETTER IOTA WITH DIALYTIKA AND TONOS, which is not encoded directly in Unicode. Suppose we decide to encode it as U+0399 GREEK CAPITAL LETTER IOTA followed by U+0344 COMBINING GREEK DIALYTIKA TONOS, which seems like a sensible way to do it. When we apply canonical decomposition, we find that the Unicode database specifies a decomposition U+0344 - it tells us that the combining mark breaks up into two characters: U+0308 COMBINING DIAERESIS and U+0301 COMBINING ACUTE ACCENT.
ä»¥å‰æ–‡æåˆ°è¿‡çš„â€œå¸¦Dialytikaå’ŒTonosçš„å¸Œè…Šæ–‡å¤§å†™å­—æ¯Iotaâ€ä¸ºä¾‹ï¼Œå®ƒæ²¡æœ‰è¢«Unicodeç›´æ¥#tr[encoding]ã€‚å‡è®¾ç°åœ¨æˆ‘ä»¬å†³å®šç”¨`U+0399 GREEK CAPITAL LETTER IOTA`ï¼ˆå¸Œè…Šæ–‡å¤§å†™å­—æ¯Iotaï¼‰å’Œ`U+0344 COMBINING GREEK DIALYTIKA TONOS`ï¼ˆ#tr[combine]ç”¨å¸Œè…Šæ–‡Dialytika Tonosï¼‰è¿™ä¸ªéå¸¸åˆç†çš„ç»„åˆæ¥è¡¨ç¤ºå®ƒã€‚å½“è¿›è¡Œ#tr[canonical]#tr[decompose]æ—¶ï¼Œæˆ‘ä»¬å‘ç°Unicodeæ•°æ®åº“ä¸º`U+0344`æŒ‡å®šäº†#tr[decompose]å½¢å¼ï¼Œè¿™ä¸ª#tr[combine]ç¬¦éœ€è¦è¢«#tr[decompose]ä¸ºä¸¤ä¸ª#tr[character]ï¼š`U+0308 COMBINING DIAERESIS`ï¼ˆ#tr[combine]ç”¨åˆ†éŸ³ç¬¦ï¼‰å’Œ`U+0301 COMBINING ACUTE ACCENT`ï¼ˆ#tr[combine]ç”¨é”éŸ³ç¬¦ï¼‰ã€‚

#let codepoint-table = (s, title: none) => {
  let cps = s.codepoints()
  block(
    breakable: false,
    width: 100%,
    align(
      center,
      table(
        columns: cps.len() + 1,
        align: center,
        if title == none {
          []
        } else {
          title
        },
        ..cps,
        [],
        ..cps.map(str.to-unicode).map(it => raw(util.to-string-zero-padding(it, 4, base: 16))),
      ),
    ),
  )
}

#codepoint-table("\u{0399}\u{0344}", title: [è¾“å…¥å­—ç¬¦ä¸²])
#codepoint-table("\u{0399}\u{0308}\u{0301}", title: [NFD])

// NFD is good enough for most uses; if you are comparing two strings and they have been normalized to NFD, then checking if the strings are equal will tell you if you have the same characters. However, the Unicode Standard defines another step: if you apply canonical composition to get characters back into their preferred form, you get Normalization Form C. NFC is the recommended way of storing and exchanging text. When we apply canonical composition to our string, the iota and the diaresis combine to form U+03AA GREEK CAPITAL LETTER IOTA WITH DIALYTIKA, and the combining acute accent is left on its own:
NFDå¯¹äºç»å¤§å¤šæ•°åº”ç”¨åœºæ™¯æ¥è¯´éƒ½æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ã€‚æ¯”å¦‚å¸Œæœ›æ¯”è¾ƒä¸¤ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ç›¸ç­‰ï¼Œåªéœ€è¦å…ˆè¿›è¡ŒNFDï¼Œç„¶åé€ä¸ªå­—ç¬¦æ¯”è¾ƒå³å¯ã€‚Unicodeè¿˜å®šä¹‰äº†å¦ä¸€ç§å¤„ç†æ–¹æ³•ï¼šåœ¨NFDçš„åŸºç¡€ä¸Šå†è¿›è¡Œä¸€ä¸ªæ­¥éª¤ï¼Œä¸ºå·²#tr[decompose]çš„#tr[character]è¿›è¡Œ#tr[canonical]#tr[compose]ï¼Œå°†å®ƒä»¬é‡ç»„æˆé¦–é€‰å½¢å¼ï¼Œè¿™æ ·å°±å¾—åˆ°äº†NFCï¼ˆNormalization Form Cï¼Œ#tr[normalization]å½¢å¼Cï¼‰ã€‚NFCæ˜¯ç”¨äºå‚¨å­˜å’Œäº¤æ¢æ–‡æœ¬çš„æ¨èæ–¹å¼ã€‚å½“è¿›è¡Œ#tr[canonical]#tr[compose]æ—¶ï¼Œå­—æ¯Iotaå’Œåˆ†éŸ³ç¬¦ä¼š#tr[combine]æˆ`U+03AA GREEK CAPITAL LETTER IOTA WITH DIALYTIKA`ï¼ˆå¸¦ Dialytika çš„å¸Œè…Šæ–‡å¤§å†™å­—æ¯ Iotaï¼‰ï¼Œå‰©ä¸‹çš„ç»„åˆç”¨é”éŸ³ç¬¦å°±åŸæ ·ä¿ç•™ï¼š

#codepoint-table("\u{0399}\u{0344}", title: [è¾“å…¥å­—ç¬¦ä¸²])
#codepoint-table("\u{0399}\u{0308}\u{0301}", title: [NFD])
#codepoint-table("\u{03aa}\u{0301}", title: [NFC])

// Note that this is an entirely different string to our input, even though it represents the same text! But the process of normalization provides an unambiguous representation of the text, a way of creating a "correct" string that can be used in comparisons, searches and so on.
è™½ç„¶è¿™ä¸ªæ–‡æœ¬å’Œæˆ‘ä»¬æœ€åˆçš„è¾“å…¥å·²ç»å¤§ä¸ç›¸åŒäº†ï¼Œä½†å®ƒä»¬è¡¨è¾¾çš„å«ä¹‰è¿˜æ˜¯ä¸€æ ·çš„ã€‚é€šè¿‡æŸç§ç‰¹å®šçš„#tr[normalization]åï¼Œç”¨æ¥è¡¨è¾¾ä¸€æ®µæ–‡æœ¬çš„å­—ç¬¦ä¸²å°±è¢«å”¯ä¸€çš„ç¡®å®šäº†ã€‚è¿™ä¸€æ–¹æ³•å¯ä»¥ç”¨äºå­—ç¬¦ä¸²æ¯”è¾ƒå’Œæœç´¢ç­‰å¤šç§åœºæ™¯ã€‚

#note[
  // > The OpenType feature `ccmp`, which we'll investigate in chapter 6, allows font designers to do their own normalization, and to arrange the input glyph sequence into ways which make sense for the font.
  æˆ‘ä»¬åœ¨#[@chapter:substitution-positioning]ä¸­ä¼šä»‹ç»çš„OpenTypeç‰¹æ€§`ccmp`å…è®¸å­—ä½“è®¾è®¡å¸ˆæŒ‰ç…§ä»–ä»¬å¸Œæœ›çš„æ–¹å¼è¿›è¡Œ#tr[normalization]ï¼Œå¯ä»¥å°†è¾“å…¥çš„#tr[glyph]åºåˆ—é‡æ–°æ•´ç†æˆåœ¨å­—ä½“å†…éƒ¨æ›´æ˜“å¤„ç†çš„å½¢å¼ã€‚

  // To give two examples: first, in Syriac, there's a combining character SYRIAC PTHAHA DOTTED (U+0732), which consists of a dot above and a dot below. When positioning this mark relative to a base glyph, it's often easier to position each dot independently. So, the `ccmp` feature can split U+0732 into a dot above and a dot below, and you can then use OpenType rules to position each dot in the appropriate place for a base glyph.
  è¿™é‡Œç®€å•ä¸¾ä¸¤ä¸ªä¾‹å­ã€‚ä¸€æ˜¯åœ¨å™åˆ©äºšè¯­ä¸­æœ‰ä¸€ä¸ª#tr[combine]#tr[character]`U+0732 SYRIAC PTHAHA DOTTED`ï¼ˆå™åˆ©äºšæ–‡å¸¦ç‚¹çš„ Pthahaï¼‰ï¼Œå®ƒç”±ä¸€ä¸Šä¸€ä¸‹ä¸¤ä¸ªç‚¹ç»„åˆè€Œæˆã€‚å½“éœ€è¦ä»¥æŸä¸ªåŸºæœ¬#tr[character]ä¸ºå‚è€ƒæ¥ç¡®å®šè¿™ä¸ªç¬¦å·çš„æ‰€åœ¨ä½ç½®æ—¶ï¼Œå¯¹è¿™ä¸¤ä¸ªç‚¹åˆ†åˆ«è¿›è¡Œå¤„ç†é€šå¸¸æ˜¯æ›´åŠ ç®€å•çš„ã€‚æ­¤æ—¶å°±å¯ä»¥ç”¨`ccmp`ç‰¹æ€§å°†`U+0732`åˆ†æˆä¸Šä¸‹ä¸¤ä¸ªç‚¹ï¼Œç„¶åå†ä½¿ç”¨OpenTypeè§„åˆ™ä¸ºåŸºæœ¬#tr[glyph]åˆ†åˆ«å®šä¹‰è¿™ä¸¤ä¸ªç‚¹çš„åˆé€‚ä½ç½®ã€‚

  // Second, the character Ã­ (U+00ED LATIN SMALL LETTER I WITH ACUTE) is used in Czech, Dakota, Irish and other languages. Unless you've actually provided an i-acute glyph in your font, you'll get handed the decomposed string LATIN SMALL LETTER I, COMBINING ACUTE ACCENT. LATIN SMALL LETTER I has a dot on the top, and you don't want to put a combining acute accent on *that*. `ccmp` will let you swap out the "i" for a dotless "Ä±" before you add your accent to it.
  ç¬¬äºŒä¸ªä¾‹å­æ˜¯#tr[character]Ã­ï¼ˆ`U+00ED LATIN SMALL LETTER I WITH ACUTE`ï¼Œå¸¦é”éŸ³ç¬¦çš„æ‹‰ä¸æ–‡å°å†™å­—æ¯Iï¼‰ã€‚è¿™ä¸ªå­—æ¯åœ¨æ·å…‹è¯­ã€è¾¾ç§‘ä»–è¯­ã€çˆ±å°”å…°è¯­ç­‰è¯­è¨€ä¸­éƒ½ä¼šç”¨åˆ°ã€‚ä¸€èˆ¬è€Œè¨€ï¼Œåªè¦å­—ä½“é‡Œæ²¡æœ‰å•ç‹¬ä¸ºå®ƒç»˜åˆ¶#tr[glyph]çš„è¯ï¼Œéƒ½ä¼šå°†å®ƒ#tr[decompose]ä¸ºå­—æ¯iå’Œé”éŸ³ç¬¦ä¸¤ä¸ªéƒ¨åˆ†æ¥å¤„ç†ã€‚ä½†æ˜¯iä¸Šé¢æœ‰ä¸€ä¸ªå°ç‚¹ï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥å°†é”éŸ³ç¬¦æ”¾åœ¨ç‚¹çš„ä¸Šé¢ã€‚è¿™æ—¶`ccmp`ç‰¹æ€§å…è®¸æˆ‘ä»¬åœ¨æ·»åŠ é”éŸ³ç¬¦ä¹‹å‰å°†å­—æ¯â€œiâ€æ›¿æ¢æˆæ²¡æœ‰ç‚¹çš„â€œÄ±â€ã€‚
]

// ## ICU
== ICU ç¨‹åºåº“

// For those of you reading this book because you're planning on developing applications or libraries to handle complex text layout, there's obviously a lot of things in this chapter that you need to implement: UTF-8 encoding and decoding, correct case conversion, decomposition, normalization, and so on.
å¦‚æœä½ é˜…è¯»æœ¬ä¹¦çš„åŸå› æ˜¯æƒ³å¼€å‘ä¸€ä¸ªå¤„ç†å¤æ‚æ–‡æœ¬å¸ƒå±€çš„åº”ç”¨æˆ–ç¨‹åºåº“ï¼Œå¯èƒ½ä¼šå‘ç°ä¸Šé¢ä»‹ç»çš„è¿‡çš„è®¸å¤šæ¦‚å¿µéƒ½éœ€è¦å®ç°ã€‚æ¯”å¦‚UTF-8çš„ç¼–è§£ç ï¼Œæ­£ç¡®çš„å¤§å°å†™è½¬æ¢ï¼Œå­—ç¬¦ä¸²çš„#tr[decompose]å’Œ#tr[normalization]ç­‰ç­‰ã€‚

// The Unicode Standard (and its associated Unicode Standard Annexes, Unicode Technical Standards and Unicode Technical Reports) define algorithms for how to handle these things and much more: how to segment text into words and lines, how to ensure that left-to-right and right-to-left text work nicely together, how to handle regular expressions, and so on. It's good to be familiar with these resources, available from [the Unicode web site](http://unicode.org/reports/), so you know what the issues are, but you don't necessarily have to implement them yourself.
Unicodeæ ‡å‡†ï¼ˆåŒ…æ‹¬å…¶æ‰€é™„å¸¦çš„é™„å½•ã€æŠ€æœ¯æ ‡å‡†ã€æŠ€æœ¯æŠ¥å‘Šç­‰å†…å®¹ï¼‰ä¸­å®šä¹‰äº†è®¸å¤šç®—æ³•ï¼Œé™¤äº†æˆ‘ä»¬ä¹‹å‰æåˆ°è¿‡çš„ä¹‹å¤–è¿˜æœ‰å¾ˆå¤šã€‚ä¾‹å¦‚å¦‚ä½•å°†æ–‡æœ¬æŒ‰è¯æˆ–è¡Œæ¥åˆ†æ®µï¼Œå¦‚ä½•ä¿è¯ä»å·¦å¾€å³ä¹¦å†™å’Œä»å³å¾€å·¦ä¹¦å†™çš„å†…å®¹å’Œè°å…±å¤„ï¼Œå¦‚ä½•å¤„ç†æ­£åˆ™è¡¨è¾¾å¼ç­‰ç­‰ã€‚è¿™äº›ä¿¡æ¯éƒ½èƒ½ä»UnicodeæŠ€æœ¯æŠ¥å‘Š#[@Unicode.UnicodeTechnical]ä¸­æ‰¾åˆ°ï¼Œä½ å¯ä»¥é€æ­¥æ¢ç´¢å¹¶ç†Ÿæ‚‰å®ƒä»¬ã€‚ç°åœ¨ä½ çŸ¥é“å¤„ç†æ–‡æœ¬æœ‰å¤šå¤æ‚äº†ã€‚ä½†è¿˜å¥½ï¼Œè¿™äº›å†…å®¹ä½ ä¸éœ€è¦å…¨éƒ½è‡ªå·±å®ç°ã€‚

// These days, most programming languages will have a standard set of routines to get you some of the way - at the least, you can expect UTF-8 encoding support. For the rest, the [ICU Project](http://site.icu-project.org) is a set of open-source libraries maintained by IBM (with contributions by many others, of course). Check to see if your preferred programming language has an extension which wraps the ICU library. If so, you will have access to well-tested and established implementations of all the standard algorithms used for Unicode text processing.
è¿‘äº›å¹´ï¼Œå¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€å¯¹æ–‡æœ¬å¤„ç†éƒ½æœ‰ä¸é”™çš„æ”¯æŒï¼Œæœ€å·®ä¹Ÿä¼šæ”¯æŒUTF-8ç¼–ç ã€‚å¯¹äºå…¶ä»–çš„é«˜çº§åŠŸèƒ½ï¼ŒICU é¡¹ç›®#[@UnicodeConsortium.ICU]æä¾›äº†ä¸€ç³»åˆ—å¼€æºåº“æ¥å¤„ç†ã€‚ICUé¡¹ç›®ç”±è®¸å¤šè´¡çŒ®è€…ä¸€èµ·åˆ›é€ ï¼Œç°åœ¨ç”±IBMå…¬å¸è¿›è¡Œç»´æŠ¤ã€‚å®ƒå®ç°äº†æ‰€æœ‰Unicodeæ–‡æœ¬å¤„ç†çš„æ ‡å‡†ç®—æ³•ï¼Œè€Œä¸”ç»è¿‡äº†éå¸¸å…¨é¢çš„æµ‹è¯•ï¼Œè¢«å…¬è®¤ä¸ºè´¨é‡ä¸Šä¹˜ã€‚ä½ å¯ä»¥çœ‹çœ‹ç°åœ¨ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€æ˜¯å¦æœ‰å¯¹ICUåº“çš„ç»‘å®šæˆ–å°è£…ã€‚å¦‚æœæœ‰çš„è¯ï¼Œé‚£é€šè¿‡å®ƒä½ å°±èƒ½ç›´æ¥ç”¨ä¸ŠICUæä¾›çš„å„ç§ä¼˜è‰¯çš„ç®—æ³•å®ç°äº†ã€‚
